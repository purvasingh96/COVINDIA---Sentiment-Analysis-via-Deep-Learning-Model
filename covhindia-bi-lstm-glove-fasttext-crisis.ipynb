{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:red; text-align:center\">COVHINDIA : Deep learning model for sentiment polarity detection of Hindi tweets related to COVID-19</h1>"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:#eb9834;text-align:center\"> EDA Pipeline</h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\"> Web Scrapping of Hindi Tweets using Twython</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install twython","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting twython\n  Downloading twython-3.8.2-py3-none-any.whl (33 kB)\nRequirement already satisfied: requests-oauthlib>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from twython) (1.2.0)\nRequirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from twython) (2.23.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.4.0->twython) (3.0.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->twython) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->twython) (1.24.3)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->twython) (2020.6.20)\nInstalling collected packages: twython\nSuccessfully installed twython-3.8.2\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the Twython class\nfrom twython import Twython\nimport json\n\n\n# Instantiate an object\npython_tweets = Twython('rWwsfBRhFgSlocEviJNVuHKot', 'PselPDHIBSb2HH3ryVKvAjoZ6IEwBI2rDpAzyzdWtpwP6gnyw9')\n\n# Create our query\ncovid19 = {'q': ['#covid19'],\n        'lang': 'hi'\n        }\n\ncoronavirus = {'q': ['#coronavirus'],\n        'lang': 'hi'\n        }\n\nindiafightscorona = {'q': ['#IndiaFightsCorona'],\n        'lang': 'hi'\n        }\n\nstayhomechallenge = {'q': ['#StayHomeChallenge'],\n        'lang': 'hi'\n        }","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndef extract_hindi_tweets_based_on(query):\n    # Search tweets\n    dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []}\n    for status in python_tweets.search(**query)['statuses']:\n        dict_['user'].append(status['user']['screen_name'])\n        dict_['date'].append(status['created_at'])\n        dict_['text'].append(status['text'])\n        dict_['favorite_count'].append(status['favorite_count'])\n\n    # Structure data in a pandas DataFrame for easier manipulation\n    df = pd.DataFrame(dict_)\n    df.sort_values(by='favorite_count', inplace=True, ascending=False)\n    return df","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_covid19 = extract_hindi_tweets_based_on(covid19)\ndf_coronavirus = extract_hindi_tweets_based_on(coronavirus)\ndf_IndiaFightsCorona = extract_hindi_tweets_based_on(indiafightscorona)\ndf_StayHomeChallenge = extract_hindi_tweets_based_on(stayhomechallenge)\n\nfinal_dfs = [df_covid19, df_coronavirus, df_IndiaFightsCorona, df_StayHomeChallenge]\neda_df = pd.concat(final_dfs)\neda_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"              user                            date  \\\n10     TheRitamApp  Tue Oct 20 04:29:37 +0000 2020   \n0       TezChannel  Tue Oct 20 04:30:35 +0000 2020   \n1       YuvaKarnal  Tue Oct 20 04:30:33 +0000 2020   \n2   COVResponseRNC  Tue Oct 20 04:30:31 +0000 2020   \n3       YuvaKarnal  Tue Oct 20 04:30:20 +0000 2020   \n\n                                                 text  favorite_count  \n10  कल(19 अक्टूबर) तक कोरोना वायरस के लिए कुल 9,61...               1  \n0   अमेरिकी संस्था 3एम द्वारा कोरोना संकट से जूझने...               0  \n1   #IndiaFightsCorona\\n\\nभूलें नहीं! अपने हाथ साब...               0  \n2   डॉ श्यामा प्रसाद मुखर्जी विश्वविद्यालय, रांची ...               0  \n3   #IndiaFightsCorona\\n\\nभूलें नहीं! अपने हाथ साब...               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n      <th>date</th>\n      <th>text</th>\n      <th>favorite_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>TheRitamApp</td>\n      <td>Tue Oct 20 04:29:37 +0000 2020</td>\n      <td>कल(19 अक्टूबर) तक कोरोना वायरस के लिए कुल 9,61...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>TezChannel</td>\n      <td>Tue Oct 20 04:30:35 +0000 2020</td>\n      <td>अमेरिकी संस्था 3एम द्वारा कोरोना संकट से जूझने...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>YuvaKarnal</td>\n      <td>Tue Oct 20 04:30:33 +0000 2020</td>\n      <td>#IndiaFightsCorona\\n\\nभूलें नहीं! अपने हाथ साब...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>COVResponseRNC</td>\n      <td>Tue Oct 20 04:30:31 +0000 2020</td>\n      <td>डॉ श्यामा प्रसाद मुखर्जी विश्वविद्यालय, रांची ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>YuvaKarnal</td>\n      <td>Tue Oct 20 04:30:20 +0000 2020</td>\n      <td>#IndiaFightsCorona\\n\\nभूलें नहीं! अपने हाथ साब...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_df.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(60, 4)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#dff51d;text-align:center\">Analysis of top users from India tweeting about coronavirus</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/input/bert-base-uncased/vocab.txt\n/kaggle/input/bert-base-uncased/config.json\n/kaggle/input/bert-base-uncased/pytorch_model.bin\n/kaggle/input/twitterdata/finalSentimentdata2.csv\n/kaggle/input/covid19-tweets/covid19_tweets.csv\n/kaggle/input/crisis-data/crisis_embeddings.text\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/fasttext/wiki.simple.vec\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/covid19-tweets/covid19_tweets.csv')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in df.iterrows():\n    y = j['text'].split()\n    if 'AarogyaSetu' in y:\n        print(j['text'])\n        break","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from geopy.geocoders import Nominatim\n\ndef extract_country(location):\n        geolocator = Nominatim(user_agent=\"geoapiExercises\")\n        if geolocator.geocode(location) is not None:\n            print('location : {}, country : {}'.format(location, (geolocator.geocode(location).raw['display_name'].split(',')[-1])))\n            return (geolocator.geocode(location).raw['display_name'].split(',')[-1])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['user_location']=='India']\ndf.head()","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"                            user_name user_location  \\\n75                InvestmentGuruIndia         India   \n76   St. Jude India ChildCare Centres         India   \n120                          Livemint         India   \n208                 Business Standard         India   \n220                               TNN         India   \n\n                                      user_description         user_created  \\\n75   our ambition is to create universal platform 4...  2015-04-03 07:39:09   \n76   A home away from home for underprivileged fami...  2012-02-29 12:21:42   \n120  Breaking news and analyses of Indian and world...  2008-11-27 09:07:38   \n208  Latest updates #news #LIVE coverage on #econom...  2009-06-01 08:52:08   \n220  Lightning fast alerts, BreakingNews, Corona Up...  2020-03-24 04:29:04   \n\n     user_followers  user_friends  user_favourites  user_verified  \\\n75            15635            35                2          False   \n76              635           371              772          False   \n120         1902888           117              472           True   \n208         1969497           380              140           True   \n220             171            74               50          False   \n\n                    date                                               text  \\\n75   2020-07-25 12:25:05  Covid to shrink power sector growth, take disc...   \n76   2020-07-25 12:25:05  #SaturdayVibes: The current situation calls fo...   \n120  2020-07-25 12:23:44  Bihar witnesses biggest single-day spike of 2,...   \n208  2020-07-25 12:20:41  From more than 1,000 deaths in the US, fourth ...   \n220  2020-07-25 12:20:24  #COVID19 in #AndhraPradesh:\\n\\n-7,813 new case...   \n\n                         hashtags           source  is_retweet  \n75                   ['Industry']  Twitter Web App       False  \n76              ['SaturdayVibes']  Twitter Web App       False  \n120                   ['Covid19']  Twitter Web App       False  \n208                           NaN        TweetDeck       False  \n220  ['COVID19', 'AndhraPradesh']  Twitter Web App       False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_name</th>\n      <th>user_location</th>\n      <th>user_description</th>\n      <th>user_created</th>\n      <th>user_followers</th>\n      <th>user_friends</th>\n      <th>user_favourites</th>\n      <th>user_verified</th>\n      <th>date</th>\n      <th>text</th>\n      <th>hashtags</th>\n      <th>source</th>\n      <th>is_retweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>75</th>\n      <td>InvestmentGuruIndia</td>\n      <td>India</td>\n      <td>our ambition is to create universal platform 4...</td>\n      <td>2015-04-03 07:39:09</td>\n      <td>15635</td>\n      <td>35</td>\n      <td>2</td>\n      <td>False</td>\n      <td>2020-07-25 12:25:05</td>\n      <td>Covid to shrink power sector growth, take disc...</td>\n      <td>['Industry']</td>\n      <td>Twitter Web App</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>St. Jude India ChildCare Centres</td>\n      <td>India</td>\n      <td>A home away from home for underprivileged fami...</td>\n      <td>2012-02-29 12:21:42</td>\n      <td>635</td>\n      <td>371</td>\n      <td>772</td>\n      <td>False</td>\n      <td>2020-07-25 12:25:05</td>\n      <td>#SaturdayVibes: The current situation calls fo...</td>\n      <td>['SaturdayVibes']</td>\n      <td>Twitter Web App</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>Livemint</td>\n      <td>India</td>\n      <td>Breaking news and analyses of Indian and world...</td>\n      <td>2008-11-27 09:07:38</td>\n      <td>1902888</td>\n      <td>117</td>\n      <td>472</td>\n      <td>True</td>\n      <td>2020-07-25 12:23:44</td>\n      <td>Bihar witnesses biggest single-day spike of 2,...</td>\n      <td>['Covid19']</td>\n      <td>Twitter Web App</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>Business Standard</td>\n      <td>India</td>\n      <td>Latest updates #news #LIVE coverage on #econom...</td>\n      <td>2009-06-01 08:52:08</td>\n      <td>1969497</td>\n      <td>380</td>\n      <td>140</td>\n      <td>True</td>\n      <td>2020-07-25 12:20:41</td>\n      <td>From more than 1,000 deaths in the US, fourth ...</td>\n      <td>NaN</td>\n      <td>TweetDeck</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>TNN</td>\n      <td>India</td>\n      <td>Lightning fast alerts, BreakingNews, Corona Up...</td>\n      <td>2020-03-24 04:29:04</td>\n      <td>171</td>\n      <td>74</td>\n      <td>50</td>\n      <td>False</td>\n      <td>2020-07-25 12:20:24</td>\n      <td>#COVID19 in #AndhraPradesh:\\n\\n-7,813 new case...</td>\n      <td>['COVID19', 'AndhraPradesh']</td>\n      <td>Twitter Web App</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_indian_sourced_tweeting_about_corona = df['user_name'].value_counts().reset_index()\ntop_indian_sourced_tweeting_about_corona.columns = ['user_name', 'tweet_count']\ntop_indian_sourced_tweeting_about_corona.sort_values(['tweet_count'], ascending=False)\ntop_indian_sourced_tweeting_about_corona.head(10)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"           user_name  tweet_count\n0    Hindustan Times          280\n1                ANI          233\n2      Deccan Herald          136\n3  Business Standard          129\n4         NewsMobile          128\n5           The Hawk           79\n6         IndiaToday           78\n7               NDTV           77\n8                ORF           62\n9   Covid India Seva           61","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_name</th>\n      <th>tweet_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hindustan Times</td>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ANI</td>\n      <td>233</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Deccan Herald</td>\n      <td>136</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Business Standard</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NewsMobile</td>\n      <td>128</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The Hawk</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>IndiaToday</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NDTV</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ORF</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Covid India Seva</td>\n      <td>61</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# import graph objects as \"go\"\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\n\n# create trace1 \ntrace1 = go.Bar(\n                x = top_indian_sourced_tweeting_about_corona.user_name[:10],\n                y = top_indian_sourced_tweeting_about_corona.tweet_count[:10],\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5)),\n                text = top_indian_sourced_tweeting_about_corona.user_name)\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\nfig.update_layout(autosize=False,\n    width=700,\n    height=500,\n    margin=dict(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4))\n                 \niplot(fig)\n            ","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"9729fa52-82ac-42f0-94b0-63db351498be\" class=\"plotly-graph-div\" style=\"height:500px; width:700px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9729fa52-82ac-42f0-94b0-63db351498be\")) {                    Plotly.newPlot(                        \"9729fa52-82ac-42f0-94b0-63db351498be\",                        [{\"marker\": {\"color\": \"rgba(255, 174, 255, 0.5)\", \"line\": {\"color\": \"rgb(0,0,0)\", \"width\": 1.5}}, \"text\": [\"Hindustan Times\", \"ANI\", \"Deccan Herald\", \"Business Standard\", \"NewsMobile\", \"The Hawk\", \"IndiaToday\", \"NDTV\", \"ORF\", \"Covid India Seva\", \"India Blooms\", \"Livemint\", \"The Logical Indian\", \"Zee News English\", \"iCanSaveLife\", \"WION\", \"DNA\", \"Behindwoods\", \"ET NOW\", \"JK 24x7 News\", \"India TV\", \"Brifly.\", \"Mirror Now\", \"The Weather Channel India\", \"99acres.com\", \"Forbes India\", \"IndSamachar News\", \"Oneindia News\", \"\\u25ab\\ufe0f\\u25fd \\ud83c\\uddee\\ud83c\\uddf3 Faiz \\ud83c\\uddee\\ud83c\\uddf3 \\u25fd\\u25ab\\ufe0f\", \"Steel Guru\", \"VJ TRAVEL SOLUTIONS\\u2122\", \"The Asian Age\", \"UCAN India\", \"The Indian Express\", \"Business Today\", \"ABP News\", \"Jan Astra\", \"CNBC-TV18\", \"Firstpost Sports\", \"THE WEEK\", \"UNICEF India\", \"ShimlaTimes\", \"EY India\", \"Gallinews.com\", \"Express Computer\", \"CNNNews18\", \"Mitalli Chandola\", \"India.com\", \"saicharan\", \"Asiaville\", \"Live Law\", \"JetSetGo\", \"Vartha Bharati\", \"#Stay_Home #Stay_Safe\", \"Naresh Yadav\", \"The Federal\", \"TheReporterPort\", \"People Helping Children\", \"Doordarshan Sports\", \"Nirmal Bhatt\", \"Banega Swasth India\", \"India's Opinion\", \"R.IOT Labz\", \"nirvachanguru\", \"Ketto\", \"The English Post\", \"Meritnation.com\", \"Down To Earth\", \"Narasimhan Narayanan\", \"PHD Chamber\", \"Sunil Malhotra\", \"Swati Vashishtha\", \"India News\", \"Newstrack\", \"SpeakTrueAlways\", \"CancerFax\", \"newsblunt\", \"SunoIndia\", \"Mohd Lateef Babla\", \"Professor M.S. Rao, PhD\", \"Jangir kishor \\ud83c\\uddee\\ud83c\\uddf3\", \"AdTrust Media\", \"The_Tall_Indian\", \"Aditi \\ud83d\\udde3\\ufe0f\", \"Sandy\", \"BloodAid\", \"Divyaprakash\", \"KPMG India\", \"NewsX\", \"The Second Angle\", \"Ketan\", \"Ortho Clinical Diagnostics\", \"Jay Singh Choudhary\", \"CASE India\", \"CareClues\", \"#Intolerant \\u092d\\u093e\\u0930\\u0924\\u0940\\u092f (Sanjeev Goyal)\", \"IndiaObservers\", \"LatestLaws.com\", \"\\u039b\\uff2b\\ud835\\ude82H\\u039b\\u3112\", \"All India Mahila Congress\", \"\\ud83c\\uddee\\ud83c\\uddf3 Pitabasa\", \"CanApprove\", \"\\u0915\\u0949\\u092e\\u0928 \\u0907\\u0928\\u094d\\u0938\\u093e\\u0928\", \"Hindustan Petroleum Corporation Limited\", \"Deloitte India\", \"InvestmentGuruIndia\", \"Mirza Hyderabad.                  (\\u0645\\u06cc\\u0631\\u0632\\u0627 \\u062d\\u06cc\\u062f\\u0631\\u0622\\u0628\\u0627\\u062f)\", \"Tanmay Das\", \"sanjeev kumar patro\", \"Ritam English\", \"Pappu ki Behan\", \"DaaruBaaz Mehta\", \"TheDialogue\", \"DTO KANGRA\", \"Shaswat\", \"NewsVoir\", \"BTVI Live\", \"Filmfare\", \"RapidLeaks\", \"The Corona Virus Eradication Project\", \"\\u265b Asjad Khan \\u265b\", \"Telangana Mata\", \"IBTimes \\ud83c\\uddee\\ud83c\\uddf3\", \"Chiranjibi Dalabehara\", \"MJ News\", \"EducationWorld India\", \"Medica Hospitals\", \"tamilnaadulive\", \"Payworld\", \"Praveen Ram\", \"Columbia India Hospitals\", \"CivicDataLab\", \"Cure SMA Foundation of India \\ud83c\\uddee\\ud83c\\uddf3\", \"Free Press Journal\", \"News Heads\", \"#cancelcompartmentexam\", \"\\ud83c\\udfaf\\ud83c\\udd7f\\ufe0fCODE\", \"Indian Feminism\", \"Revive Counselling Services\", \"Kumar Manish #VoicesAgainstCovidStigma\", \"Ravi Shankar Prasad\", \"FinSec\", \"Kiran Kumar Goli\", \"Reader\\u2019s Digest India\", \"Keshav Kumar \\ud83c\\uddee\\ud83c\\uddf3\", \"Ngurang Reena\", \"CITY NEWS MUMBAI\", \"Zoom News\", \"Sangath\", \"Oigetit Fake News Filter India\", \"knayha mongha\", \"Times Drive\", \"Alok Taunk\", \"MenRightsIndia\", \"\\u0950 M \\u0950\", \"Covid India Stats\", \"Blood Doner O+\", \"Tanisha Gupta\", \"Desimartini\", \"Amit Malewar\", \"Sports.info\", \"Lav Kush\", \"Feminism in India\", \"Tech Explorist\", \"The Scrbblr\", \"Twitter Moments India\", \"Abhishek Singh \\ud83c\\uddfa\\ud83c\\uddf3\\ud83c\\uddee\\ud83c\\uddf3\", \"Vijay Kumar S\", \"Xpheno\", \"India Ahead News\", \"\\u042fohit Abhyankar\", \"Anna!\", \"Major General PC SEN\", \"Vishal Chaturvedi\", \"Saurabh Yadav\", \"Sameer Panda\", \"Indiastat\", \"Sreejith Kumar S \\ud83c\\uddee\\ud83c\\uddf3\", \"Analytics Insight\", \"Sohaiil A Merchant\", \"Tribal Army\", \"Prashant Singh (Zee Business Digital)\", \"Khel Now\", \"Babu Nuvu Btech ah? - BNBA\", \"Doctor Roshan R \\ud83c\\udf0d\", \"NYKS India\", \"OnlineBloodDonors\\ud83c\\uddee\\ud83c\\uddf3\", \"DataLEADS\", \"MahindraFirstChoice\", \"NewzIt\", \"Siddharth Jain\", \"TEJAS\", \"RSCommunication\", \"Stackorigin\", \"Kumar Manish #StayAtHome \\ud83c\\udfe1\", \"NilayaResort\", \"Mahesh Joshi\", \"Kiran\", \"Adity Saxena, PhD\", \"TNN\", \"Digital Terminal\", \"Brainfeed Magazine\", \"HospitalsApollo\", \"Jaspal S\", \"INCian\", \"C/o Controversy\", \"ATZone\", \"\\ud83c\\uddee\\ud83c\\uddf3 \\u03ac\\u03bb\\u03c6\\u03b1 \\u03c0\\u03c1\\u03c9\\u03c4\\u03b1\\u03c1\\u03c7\\u03b9\\u03ba\\u03cc \\ud83c\\uddee\\ud83c\\uddf3\", \"PRPocket\", \"The Opinions\", \"Aon India Consulting\", \"#VishhalThakkaar\\u00ae\\ud83c\\uddee\\ud83c\\uddf3\", \"IITians 4 India\", \"Pratiba Raman\", \"Sri Chaitanya Educational Institutions\", \"India Water Portal\", \"Fortis Healthcare\", \"Venu K N\", \"Heros Foundation \\ud83c\\uddee\\ud83c\\uddf3\", \"Vilcity India\", \"Mahi\", \"Lakshmi Ronanki\", \"StillRising\", \"Jayanth P\", \"travelobiz\", \"Summachar - News that Matters\", \"Pavitra Trambadia\", \"Dr Deepali Bhardwaj\", \"Ziqitza HealthCare Ltd.\", \"SantaBanta.com\", \"Covid SOS\", \"earnmoneylife\", \"NISA Education\", \"OwlSpeaks\", \"Thoothukudi Kaaran\", \"Dr Neelam Tewari\", \"Doc MCS \\ud83c\\uddee\\ud83c\\uddf3\", \"jiGNONYM\", \"Santati Fertility\", \"SAGE Publications\", \"Manish Chawla\", \"Susheel\", \"ETCISO\", \"World Animal Protection India\", \"Gagan Gulyani\", \"Credence HR Services\", \"Raghavendra\", \"Prakhar Singh\", \"Indian Group of Automation\", \"Shanti Bhushan Roy\", \"E-TUITIONS\", \"C3 India\", \"MedifenceLLp\", \"Clout News\", \"BL Kashyap\", \"Keka HR\", \"JioTV\", \"Indian Pharmaceutical Alliance\", \"ISMG APAC & ME\", \"ASD Ad Media Solutions\", \"Jitender Kumar\", \"Kashif Jamal\", \"puneet\", \"Don't Prick\", \"Open PICU\", \"Kamal Preet \\ud83d\\udd1c #E2 \\ud83c\\udde6\\ud83c\\uddfa\\ud83d\\udc28\\ud83e\\udd98\", \"Rd\", \"Dr.Johns / \\u0d21\\u0d4b.\\u0d1c\\u0d4b\\u0d7a\\u0d38\\u0d4d\", \"FIDR |Stay Home\\ud83c\\udfe0 Stay Safe\\ud83d\\ude4f\\ud83c\\udffb\\ud83d\\ude4f\\ud83c\\udffb|\", \"Incubig\", \"Shashank \\ud83c\\uddee\\ud83c\\uddf3\", \"Pycker Tollywood\", \"Kiq\", \"Rajesh Dubey\", \"Nat24\", \"J-PAL South Asia\", \"SearchnScore\", \"CII Live\", \"Amita Chauhan\", \"Open Source For You\", \"momspresso.com\", \"Tanisha Gupta \\ud83d\\udd8a\\ufe0f\", \"DareDevil\", \"Splainer\", \"follo\", \"Bholanath Acharya\", \"Yashvardan Sahu\", \"Anil Khera - Advisor, Consultant\", \"Narsi Benwal\", \"Manasvi Thapar \\u092e\\u0928\\u0938\\u094d\\u0935\\u0940 \\u0925\\u093e\\u092a\\u0930 \\ud83c\\uddee\\ud83c\\uddf3\", \"People Matters\", \"UK in India\\ud83c\\uddec\\ud83c\\udde7\\ud83c\\uddee\\ud83c\\uddf3\", \"Victor\", \"Covid Tracking India\", \"The-Pulse\", \"Waseem khan\", \"N.Chandrasekar\", \"Adarsh Burman\", \"Nisus Finance\", \"Faraz Ahmad\", \"Vikram Singh\\ud83c\\udd9a\\ufe0f\", \"Kaptiche\", \"HelpAge India\", \"Shekhar\", \"NCERT\", \"Eagle eye\", \"T S Sudhir\", \"\\ud83d\\udc26 Saurav \\ud83c\\uddee\\ud83c\\uddf3\", \"Commerce News Guruji\", \"Mohammed Zubair\", \"\\ud83c\\udd71\\ufe0f\\ud83c\\udd70\\ufe0f\\ud83c\\udd71\\ufe0f\\ud83c\\udd70\\ufe0f....\\u24c2\\ufe0fSDian\", \"\\u211d\\ud835\\udd52\\ud835\\udd67\\ud835\\udd5a\", \"The GODFATHER\", \"RadioandMusic.com\", \"Medanta\", \"Pratibha {Pratsmusings}\", \"AnandT\\u2122\", \"mint_lounge\", \"Toybank - Development through Play\", \"JRK Group\", \"Prem Anand Murugan\", \"Jawad\", \"Ganesh Aradhya BN\", \"The Zucker Doctor\", \"Iamindian\", \"Ajith Mathew\", \"creative canvas\", \"Aarti\", \"\\u0905\\ud835\\udd9b\\ud835\\udd8e\\ud835\\udd93\\ud835\\udd86\\ud835\\udd98\\ud835\\udd8d\", \"Thaneesh_tani\", \"Info Captain\", \"Prashant Kumar\", \"#Bhagwa-E-Hind \\ud83d\\udea9\", \"YourStory\", \"Manish MMK\", \"\\u1e32uc\\u1e27 \\ud83d\\ude4a Bh\\u00ef\", \"Pradeep Kumar Jain\", \"Anil Kumar Dhawn\", \"End AIDS India\", \"Abdul\", \"bhaskar\", \"G \\ud83d\\udcb8\", \"Manish Gupta\", \"Technology Counter\", \"Weizmann Institute Of Science Desk In India\", \"Dhaval Khatri DK\", \"Sandeep Shetty\", \"Sun Direct\", \"Welspun Group\", \"#vimal@kant\", \"Child Rights and You\", \"Official Hitendra Sampat\", \"Diabetes Health\", \"Data Governance Network\", \"Shalini. S. Nair\", \"Haqdarshak\", \"Salil Maheshwari\", \"Confederation of Indian Industry\", \"CaseReads\", \"Ankit\", \"Rameez Makhdoomi\", \"Shaune B. Ryder\", \"Smart Family\", \"slave of Bharat mata \\ud83d\\ude47\\ud83d\\ude47\\ud83d\\udea9\\u270a\", \"Manish Kumar Mishra\", \"Gaurav Jain\", \"Surendar K\", \"Indrajeet Singh\", \"ASHRAF\", \"Aishwarya\", \"SK\", \"Follicare Hair Solution\", \"vivek kumar Maurya\", \"Filmy Sansaar\", \"Shivani Mishra\", \"Shivani Yadav\", \"u\\u0250\\u0131\\u0279\\u01dd\\u0287\\u0131\\u0253\\u0253\\u018e\", \"The Fearless Male \\ud83d\\udc90 \\ud83d\\ude0b\", \"Mister Rummy\", \"Flingkin\", \"Dr. Ashwani Kumar\", \"Madhur Sharma I \\u092e\\u0927\\u0941\\u0930 \\u0936\\u0930\\u094d\\u092e\\u093e\", \"Guide Runners India\", \"Srujana.D\", \"#StayHome Amit Mishra\", \"AIM\", \"Saksham Bansal\", \"thepatsala\", \"Syed Ali\", \"Rohini Mohan\", \"Sanjay Shridhar\", \"Apoorv Jain\", \"Soumya\", \"JayeelovesTravel\", \"Sujit Kumar jena\", \"DefenceAviationPost\", \"The Kbeat\", \"Piyush Ranjan Dubey\", \"Geet\", \"Dash\", \"Manoranjan Bag\", \"CARE-FOR-ECOSYSTEM\", \"The Common Column\", \"News Now Nation\", \"Dr.Rahman.Chaudhary\\ud83c\\uddee\\ud83c\\uddf3\", \"Anil Kapoor\", \"Ashok Malik\", \"NatGeoTravellerIndia\", \"OGenie\", \"Kashyap Media\", \"nazish\", \"Rajeev Jain @gallerygrandeur\", \"Congress will come back to Power\", \"Lemuria_memes\", \"Traffic Sahayak\", \"Skylar\", \"\\u039b\\uff2bSH\\u039b\\u3112\", \"PRAMOD RAGHAVAN\", \"Rudra Trilogy\", \"Faceless Compliance\\u2122\", \"Flipkart Stories\", \"MEERSIDDIQALI\", \"bhishm\", \"Cond\\u00e9 Nast Traveller\", \"Sachin Negi Aap\", \"Manohar\", \"Easy Shiksha\", \"Mobulous Technologies Pvt Ltd\", \"Freelancertohire.com\", \"Krithiga Narayanan\", \"CricketNext\", \"Krishna\", \"Humsafar Weddings\", \"THE NAVEEN DUBEY (TND)\", \"Shadowfax\", \"Taher\", \"Growdiesel Ventures Limited\", \"Vishesh\", \"\\u0938\\u094b\\u0939\\u0928 \\u0917\\u094b\\u0938\\u094d\\u0935\\u093e\\u092e\\u0940 \\ud83c\\uddee\\ud83c\\uddf3\", \"Trading is fun\", \"Satish Kapur\", \"Superpower Football\", \"TheLifeofScience.com\", \"Rashmika Mandanna ALL INDIA\\ud83c\\uddee\\ud83c\\uddf3 FC\\u2764\\ufe0f\", \"SheThePeople\", \"Nagendra Singh Yadav\", \"Er.Syed Arsh \\u0639\\u0644\\u06cc\", \"ASCI 77100 12345\", \"\\ud83c\\uddee\\ud83c\\uddf3gujju_\\u0935\\u0949\\u0915\\u0930\", \"Jokin Jeyapaul\", \"Priya Thakur\", \"Sab Janta\", \"Zohari\", \"Gagan Talwar\", \"NavaBharat\", \"\\u091c\\u092f \\u0939\\u093f\\u0928\\u094d\\u0926\", \"\\ud83d\\udd34jasvinder Singh\", \"Neena Dayal \\u23e9\", \"\\ud83c\\uddee\\ud83c\\uddf3  Riitu Chugh \\u090b\\u0924\\u0942 \\u091a\\u0941\\u0918\", \"Friends Of Max\", \"Gurjit Singh\", \"Urvi kar\", \"Green Tea Films - Shilpa Medicare\", \"ob_servant\", \"Deepak Patel\", \"Everything Experiential\", \"Life Is A Race\\ud83d\\udca1\", \"Traveller\", \"threshold20n\", \"Edelweiss Group\", \"\\ud835\\udd3b\\ud835\\udd3c\\ud835\\udd3c\\u2119\\ud835\\udd38\\ud835\\udd42 \\u211d\\ud835\\udd38\\ud835\\udd41\\u2119\\ud835\\udd4c\\ud835\\udd4b\\ud83c\\uddee\\ud83c\\uddf3\", \"Know-Todays-News\", \"\\ud835\\udc18\\ud835\\udc28\\ud835\\udc06\\ud835\\udc22\\ud835\\udc24\\ud835\\udc2b\\ud835\\udc22\\ud835\\udc2c\\ud835\\udc21\\ud835\\udc27\\ud835\\udc1a \\ud835\\udc05\\ud835\\udc28\\ud835\\udc2e\\ud835\\udc27\\ud835\\udc1d\\ud835\\udc1a\\ud835\\udc2d\\ud835\\udc22\\ud835\\udc28\\ud835\\udc27\", \"Q Live Media\", \"Kool Kanya\", \"Kanika\", \"Rafa\\ud83d\\udc99\", \"Monika\", \"Better J&K\", \"Rapid Response\", \"India CSR Network\", \"CBRE India\", \"V Vangala\", \"Tarun. P\", \"OPaL\", \"Mushtaq Ansari \\ud83c\\uddee\\ud83c\\uddf3#PotholeWarriors #FeedTheNeedy\", \"Vats\", \"Reclaim my India. Linysajan\", \"Khaleequr Rahman\", \"JB Chemical Works\", \"Sunjoy KRG\", \"Suppressed Soul\", \"Roam1 Telecom\", \"YESHPAL SINGH TOMER\", \"Industry Live\", \"Muthoot Microfin\", \"Gayathri\", \"Avinash Kadeshivalaya\", \"St. Jude India ChildCare Centres\", \"India Forums\", \"Tamojit\", \"Shirshendu Ghoshal\", \"Joydeep Phukan\", \"Prayogshala\", \"M\\u1d5c\\u0101z Kal\\u012bm\", \"Rising hindu\", \"Abhishek Gupta\", \"Jakson Group\", \"Prathamesh_extc\", \"Yogi \\u092f\\u094b\\u0917\\u0940\", \"Pankaj Kumar\", \"King Sulthan\", \"Yuktika Kashyap\\ud83c\\uddee\\ud83c\\uddf3\", \"I Support Reservation\\u300aISR\\u300b\", \"Indrajeet singh\\ud83c\\uddee\\ud83c\\uddf3\", \"Sebin T\", \"TheBathroomWriter\", \"RUDRATH.DESAI\", \"GNSG\\ud83c\\uddee\\ud83c\\uddf3\\ud83c\\uddee\\ud83c\\uddf3\", \"T Francis\", \"SOPHIA HANDA \\ud83e\\uddda\\ud83c\\udffc\\u200d\\u2640\\ufe0f\\u0633\\u0648\\u0641\\u06cc\\u0627\", \"Dhanaraj Thosare\", \"DS\\ud83c\\uddee\\ud83c\\uddf3\", \"India: secularism not found\", \"Rosme Dubey\", \"Filmy Gupshup\", \"Rajiv Chopra\", \"Sachin Sonekar\", \"Absolute India\", \"Papri Banerjee\", \"Adv. Ansh Tewatia\", \"Kunal Shekhar\", \"R.M.MADHUMARAN\", \"JATs World\\ud83c\\udf0e\\ud83c\\uddee\\ud83c\\uddf3\", \"rajesh minia\", \"ILO India\", \"Hem \\ud83c\\uddee\\ud83c\\uddf3 \\ud83d\\ude4f\\ud83c\\udffc\", \"Rajdip Barua\", \"Nalini Kaushal\", \"bankagent\", \"@n1l B15ht\", \"#IndiaFirst\", \"Saurav Saha\", \"WeddingWire India\", \"Carve Startup Labs\", \"Dr. Sarit K. Das \\ud83c\\udf40\", \"Save Aarey\", \"Akash Goila Official\", \"DataSasi\", \"Yash | The #DigitalMarketing Pro\\ud83d\\ude80\", \"Crypto Kanoon\", \"CorpGovern\", \"\\ud835\\udc6a\\ud835\\udc76\\ud835\\udc7d\\ud835\\udc70\\ud835\\udc6b 19\", \"OnGraph Technologies\", \"Padam Shukla\", \"vanakkam\", \"Jindal Panther\", \"kiara\", \"Simply Life Tips\", \"HealthLappy\", \"reliancesmartmoney.com\", \"Ratish\", \"Avon India\", \"Lucifer\", \"Nandini\", \": K .\", \"Furkhan \\ud83c\\udf65\", \"Newton Bank Kumar\", \"NewsBits.in\", \"Bond High Plus\", \"Funky Govind\", \"Surya Guptha - Born for Farmers\", \"Mona Ambegaonkar\", \"Chandra Prakash\", \"Doofy - Sakkyboy\", \"Eshaan Amte\", \"the indian national daily\", \"Ankit Tyagi\", \"Steve Rogers\", \"Sriyan Official Group\", \"Taru - \\ud83c\\udf33\", \"Televed - First Line of Care\", \"The Skeptical Indian\", \"Natio_First \\ud83d\\udd49\\ufe0f\", \"Shivya Nath\", \"Somi Srivastava\", \"Misty Maity\", \"DAperspectives\", \"\\u0b85\\u0ba8\\u0bbe\\u0bb9\\u0ba4\\u0bae\\u0bcd\", \"\\ud83d\\udd4a\\ud83c\\udf0d\\ud83d\\udd4a\", \"Mandar Bhide\", \"RAJDEEP DEB \\u201cYUVA CONGRESS\\u201d\", \"Simranpreet Oberoi\", \"\\u0927\\u0930\\u094d\\u092e \\u0938\\u0930\\u094d\\u0935\\u092a\\u094d\\u0930\\u0925\\u092e\", \"Thirumalaisamy K\", \"Swaraj Mart\", \"joel\", \"Shyam\", \"Padmaja Pawar-Yadav\", \"Motivationworld\", \"Bharath Balakrishna\", \"HANSRAJ\", \"Shailesh Tripathi\", \"Jazz Soni\", \"Navin Tyagi\", \"Clarion India\", \"Korah Abraham\", \"Sangam\", \"Mumbhaikar\", \"Feed A Million \\ud83c\\uddee\\ud83c\\uddf3 #WeCan\", \"WRI India\", \"Shivam Shukla\", \"Swachh Bharat I #IndiaFightsCorona\", \"Sen\", \"Yogesh\", \"Sanjeeta KK\", \"Baba NR\", \"Shantiswarup Mahapatra \\u00a9\", \"INDIA OUTBOUND MAGAZINE\", \"Pranav Das\", \"Udit Mohan\", \"Pushparaj Deshpande | \\u092a\\u0941\\u0937\\u094d\\u092a\\u0930\\u093e\\u091c \\u0926\\u0947\\u0936\\u092a\\u093e\\u0902\\u0921\\u0947 | \\u0aaa\\u0ac1\\u0ab7\\u0acd\\u0aaa\\u0ab0\\u0abe\\u0a9c\", \"HealthDNA\", \"Taxmann\", \"Rohith Raj\", \"Gaysi Family\", \"Medindia.net\", \"SBS Fin Advisory\", \"TicketNew\", \"Alex jhon\", \"Kunal D Saikia\", \"Ideas Design Solutions (P) Ltd.\", \"NewsFreak 2.0\", \"BallyhooToday\", \"RJ\", \"Ashwin Sudhakaran\", \"Kiran Kumar Shrivastav\", \"Deeksha Talwar\", \"DigiMantra Labs\", \"Srinivasan Sriram \\ud83c\\uddee\\ud83c\\uddf3\", \"AS-IT-IS-Nutrition\", \"Farhan\", \"Selim Akhtar\", \"Job Junction\", \"Anamika Tiwari\", \"Dr.SHREYAS BS\", \"WFM Media\", \"Pratik\", \"Geojit Financial\", \"Indian Khan\", \"Namaste UI\", \"RAYZONIK\", \"filmybaapOfficial\", \"NewsMen\", \"IIHM\", \"Flourishing Media House\", \"RenewBuy.com\", \"Varun Kumar \\ud83c\\udfb6\", \"Oxfam India\", \"Dr. OBGYN \\uf8ff\", \"FUTUREISONEARTH\", \"Ashish\", \"WHITEPEAK Consulting\", \"Aryan Kedia\", \"Maven Overseas\", \"shabaz pasha\", \"VIKASH KUMAR\", \"Prisom Technology LLP\", \"ISCR\", \"Alisons Infomatics\", \"Namya Joshi\", \"Raats\", \"Suchiradipta\", \"Ashish Bhasker\", \"Saleem Z.A.\", \"S H \\u00cf \\u00d1 \\u00cf G A M \\u00ce \\ud83c\\udf4e\", \"Dr Dharmendra Singh Rajput\", \"Anmol V\", \"ElectronicsForYou\", \"GurGaon\", \"\\u0915\\u093f\\u0930\\u0923 \\u091a\\u0935\\u094d\\u0939\\u093e\\u0923\", \"Nandhupriya\", \"Happieshop4u Online Shopping India\", \"At Tamil\", \"Animesh Kumar / \\u0905\\u0928\\u093f\\u092e\\u0947\\u0937 \\u0915\\u0941\\u092e\\u093e\\u0930 \\ud83c\\uddee\\ud83c\\uddf3\", \"A&A Business Consulting - OYB\", \"SANDEEP\", \"The August\", \"PrettyLocust\", \"Ramachandra\", \"ZNetLive\", \"The Representative\", \"mergersindiainfo\", \"Projects Today\", \"Pratik Maity\", \"Mamata Banerjee Supporters (FAM)\", \"Nanjundan T\", \"Ram Vilas Ghosh A\", \"Deepak\", \"Bihar NEWSBOX (Hindi/English)\", \"Deepankar Pathak\", \"Krishna Kumar\", \"Prachi Singh Chaturvedi \\ud83c\\uddee\\ud83c\\uddf3\", \"Aritry Das\", \"\\u091c\\u093e\\u091f \\u0926\\u093e \\u092a\\u0941\\u0924\\u094d\\u0924 \\ud83d\\udc73\", \"Hardeep Singh\", \"Pressbrief \\ud83d\\udcf8\", \"PepPill\", \"Uday Deb\", \"MISSING\", \"Saumya\", \"Abhi Upadhyay\", \"Rohan J. Chhajed\", \"Security and Forces Update\", \"Megha Rajoria\", \"Manpreetk\", \"Core Integra\", \"vinay\", \"IndiaDataHub\", \"Mulberry Lifestyle\", \"Sarvana\", \"Nitin Srivastava\", \"HT Entertainment\", \"FilmyKeema Buzz\", \"FrontList\", \"Life Bee\", \"MobDhoom\", \"Shuma\", \"Lekhika\", \"Tehelka\", \"Yash Tiwari Speaks\", \"NGOBOX-Grants, Jobs, Fellowships and More\", \"INECC\", \"IPL 2020 \\ud83c\\udde6\\ud83c\\uddea\", \"KUBER\", \"VISHNU\", \"\\ud835\\udcdf\\ud835\\udcea\\ud835\\udcf7\\ud835\\udcf4\\ud835\\udcea\\ud835\\udcf3 \\ud835\\udcdf\\ud835\\udcea\\ud835\\udcfb\\ud835\\udcf2\\ud835\\udcf4\\ud835\\udcf1\", \"Radhey raman\", \"Er.DURGA PRASAD DWIVEDY\", \"\\ud83e\\udde1Jheel\\ud83e\\udde1\", \"Rajesh Purohit\", \"Vivek pandey\", \"Infinite MLM soft\", \"mxxx\", \"socialmantraa\", \"SportzCraazy\", \"\\ud835\\udde3\\ud835\\uddff\\ud835\\uddee\\ud835\\uddef\\ud835\\uddf5\\ud835\\uddee\\ud835\\ude00 \\ud83e\\udde2\", \"Ranjith Kannan Raghavan\", \"\\u0915\\u0941\\u0932\\u0930\\u093e\\u091c \\u092b\\u093c\\u094c\\u0917\\u093c\\u093e\\u091f (\\u090f\\u0915\\u094d\\u0938 \\u0906\\u0930\\u094d\\u092e\\u0940)\", \"kaam_de\", \"Aman Parakh \\ud83c\\uddee\\ud83c\\uddf3\", \"Perfect Woman India Official\", \"Artholique\", \"Prashant Bhangale\", \"Natiq Haque | \\u0928\\u093e\\u0924\\u093f\\u0915\\u093c \\u0939\\u0915\\u093c | \\u0646\\u0627\\u0637\\u0642 \\u062d\\u0642\", \"First Partners\", \"Anirban Basu\", \"Kumar Manish\", \"Arvindraj\", \"EquateAll \\ud83c\\uddee\\ud83c\\uddf3\", \"Rex\", \"Neeraj Kumar\", \"Tajuddin\", \"Fans of KOLLEO (Saroop Chattopadhyay)\", \"Logically India\", \"Ek Niranjan\\u2764\\ud83d\\ude0e\", \"Rituparna Ghosh\", \"Neha Karnad\", \"Citizen Matters\", \"Vishal\", \"DrMightyChondrion\\u2122 #PutAMaskOn\", \"Aditi\", \"Indrajeet Chandra\", \"7 WAVES\", \"Arun\", \"financial-markets\", \"Amit Kumar\", \"Harshdeep Anand\", \"Skyram Technologies\", \"\\ud83c\\uddee\\ud83c\\uddf3Stuti\\ud83c\\udf0f\\ud83c\\udf34\\ud83d\\udca6\\ud83e\\udd8b\", \"Abhishek\", \"AccountabilityIndia\", \"Tucareers\", \"Anamika \\ud83c\\uddee\\ud83c\\uddf3\", \"Abdul Matheen\\ud83c\\uddf3\\ud83c\\uddea\", \"Dr. Shahid Siddiqui\", \"TruckSuvidha\", \"Kohinoor Chakraborty\", \"M3 India Oncology\", \"Siju Moothedath\", \"Uday Bhan\", \"Salil Shetty\", \"YOUR RETAIL COACH\", \"Abhijit Ghosh\", \"H@\\u00ae\\u26a1H\", \"Nayan Patel \\ud83c\\uddee\\ud83c\\uddf3\", \"Cinema Bugz\", \"\\u262f \\ud835\\udd75\\ud835\\udd86\\ud835\\udd8c\\ud835\\udd8e\\ud835\\udd97 \\ud835\\udd7d\\ud835\\udd86\\ud835\\udd94 \\u0131ll\\u0131ll\\u0131 \\u0924\\u092e\\u0938 \\u0930\\u093e\\u091c \\u0131ll\\u0131ll\\u0131\\u2623\\ufe0f\", \"Times OF Press\", \"Love,Uttkarsh\", \"R.M Tiwari\", \"Rahul Gupta\", \"MapsofIndia\", \"Apne Aap\", \"Dev Ashish\", \"PPHF_Asia\", \"DocStokesindia\", \"Vindu Dara Singh\", \"Gopal Jspk\", \"ETNOW+\", \"ZERO TOLERANCE\", \"ABHISHEK\", \"Karan Bhasin\", \"Prama Sarkar\", \"ranjeet kumar sinha\", \"Supreme Wart of India\", \"Daily Documentary\", \"Warrior\", \"Rachna Tandon\", \"The HealthCare IT Experts Blog\", \"Skyscraper Solutions\", \"Vishal.\", \"Nikit Patel\", \"FoodFood\", \"IDU\", \"\\ud835\\udc15\\ud835\\udc22\\ud835\\udc23\\ud835\\udc1a\\ud835\\udc32\", \"Pulkit Gupta\", \"migrant labour\", \"Sumit Joshi\", \"A K Jha\", \"Nilkanth Donga\", \"CSR Live\", \"Aditya Choudhary\\ud83c\\uddee\\ud83c\\uddf3\", \"blinkbloomdesign\", \"Corporate Yogi\\u2122\", \"Scherezade Shroff\", \"Current Affairs \\ud83c\\uddee\\ud83c\\uddf3\", \"Samir\", \"Neha Bhan\", \"pluspromo\", \"Crossword PR\", \"Redbox India\", \"WRMS\", \"Tanuj Singh\", \"Krishna vamsi\", \"afaqs!\", \"ABD\", \"Vijay Veer Singh Chhonkar\", \"Amar Prasad Reddy\", \"ABC Live India\", \"You may call me V\", \"TechAP\", \"Deepak Kumar\", \"Tushar Shahi\", \"AB\", \"Narayan Lakshman\", \"Arijit Mukherji\", \"IndianWeb2\", \"Tech Mahindra Foundation\", \"MVKSTRONG11\", \"DIGITALEDU4KIDS\", \"Braj\", \"CA Ashmita Debnath\", \"Nikhil Inamdar\", \"Janmejay Patel\", \"Swamitwitanand\", \"Coach Prashant\", \"Startup Incubation and Innovation Centre, IITK\", \"Surfaces.Reporter\", \"Stock Charts\", \"InceptiveMind\", \"Stay At Home \\ud83d\\ude37\", \"Shouvik Majhi\", \"Yo!Kart\", \"\\u0906\\u0924\\u094d\\u092e\\u0928\\u093f\\u0930\\u094d\\u092d\\u0930 \\u0905\\u0926\\u0943\\u0936\\u094d\\u092f\", \"Harnil Oza\", \"Sakshi\", \"Peace\", \"Dr.Mehbub Ali Ahmed\", \"N.K.Chanji\", \"Om Jadhav \\ud83c\\uddee\\ud83c\\uddf3\", \"Capt Pawan Kalkal (Veteran)\", \"Vanshika Gupta\", \"Corona Warrior Tejal Mali\", \"Parth Sawant\", \"JK Technosoft\", \"Change.org India\", \"Cherry M P\", \"My Choices Foundation\", \"Hiteshmishra\", \"360researchfoundation\", \"Responsenet\", \"Rahul Chauhan\\ud83c\\uddee\\ud83c\\uddf3\", \"HealthyLifestyle4us\", \"Sailaja Akkala\", \"CurateSport\", \"Syed Rizwan Qadri\", \"Garabh Satya\", \"TECHGIG\", \"RENTTRY - Bikes on Rent\", \"Chandrakanta dalai\", \"Dr. Dheeraj Agrawal\", \"Anurag\", \"Prudvinadh Reddy\", \"Shubham Dubey\", \"Kalpak Kekre\", \"Ishita Singh\", \"Simi Chakrabarti\", \"Anaz\", \"Shalini\", \"Heena Varun Khera\", \"Toshi Automation Solutions\", \"Sandeep Naharia\", \"Madhav\", \"Harsha Sayings\", \"Ritesh\", \"sasi \\ud83c\\uddee\\ud83c\\uddf3\", \"Namrata Arora\", \"Dharini Parthasarathy\", \"Grazia India\", \"Jaquar\", \"Neeraj Garg\", \"ankit sodia\", \"Aakaash\", \"INTV Media\", \"subroto\", \"GST Helpline\", \"Hindi Lyrics World\", \"Edmund Samuel\", \"Censor Info\", \"sunil maheshwari\", \"omnath sharma\", \"ClicksWorthy\", \"Sunil Saini\", \"Rameesh Kailasam\", \"Maggie\", \"Changing Tomorrow\", \"Way 2 Meds\", \"Priyanka_BSNL\", \"All SOCIAL MEDIA WORLD\", \"Piyush\", \"\\ud835\\udc6a\\ud835\\udc76\\ud835\\udc7d\\ud835\\udc70\\ud835\\udc6b 19 \\ud83c\\uddee\\ud83c\\uddf3\", \"Online Tamil Junction\", \"Avijeet Rathsharma \\ud83c\\uddee\\ud83c\\uddf3\", \"Infoblaze INDIA\", \"Asiem Atriy Fitness\", \"Meri Bacchs \\u2764 ka budday \\ud83e\\udd73\", \"India Employer Forum\", \"BASANT KUMAR SINGH\", \"KeepItPumping India\", \"Ved Prakash\", \"ClearTax.in\", \"Shreevatsa Nevatia\", \"Om Narayan Rai\", \"Priyanka\", \"India Power Talk\", \"Meena Vaidyanathan\", \"The SEOTalkers\", \"Chintan\", \"ANURAG SABHARWAL\", \"Anindita Chatterjee\", \"media.syndicate\", \"Naveen Pragada\", \"\\u26a1\\ufe0fBhupendra Bohra \\u26a1\\ufe0f\", \"sai\", \"Sambit Pal\", \"Moksh Coaching\", \"Nitin \\ud83c\\uddee\\ud83c\\uddf3\", \"Vikrant $eth\", \"\\u00c1J\", \"mhospitals\", \"Jayaditya\", \"AGWO\", \"Dr. Vishwajeet Kadam\", \"Amit Gupta\", \"Daljeet Singh Brar\", \"Utkarsh Vashistha \\ud83c\\uddee\\ud83c\\uddf3\", \"Deepak Marathe\", \"Chandana S Anand\", \"Gandharv Kamala\", \"DR TUSHARKANTI NAYAK\", \"hafeezullah kv\", \"Pradeep Kumar\\ud83c\\uddee\\ud83c\\uddf3\", \"Martytholath\", \"Janardhan Koushik\", \"India Culture Lab\", \"The chosen Saffron\", \"And Stories\", \"Retail Pharma India\", \"VERSATILE\", \"Nitin Mishra\", \"BSNL \\u0915\\u093e \\u091c\\u093e\\u0901\\u0917\\u093f\\u095c\", \"New India World\", \"Jai Singh\", \"Mr Manjur \\ud83c\\uddee\\ud83c\\uddf3\\ud83d\\ude4f\", \"Krish TechnoLabs\", \"Kashif\", \"Chandra sekhar \\ud83c\\uddee\\ud83c\\uddf3\", \"arvind\", \"Aftab Ahmad\", \"Sunny Geo\", \"League of India @ #IndiaFightsCorona\", \"Ankita Pandey\", \"Anindya Kar\", \"Kaya Clinic\", \"Recruise India Consulting\", \"AutoTalk\", \"CIMGLOBAL\", \"Ashutosh Srivastava\", \"Skribe\", \"Ayushi P\", \"Atharva\", \"Prakash Tyagi\", \"366Pi Consulting - Full Spectrum Growth Services\", \"VoxPopuli\", \"#5thAugust2020\", \"CSRBOX-Impact First\", \"Gaurav\", \"Abhishek Sarkar\", \"Suresh Prabhu\", \"Priyanshu choudhary\", \"Current Affairs for Bank, SSC & Railway Exams\", \"Praveen Agrawal \\ud83c\\uddee\\ud83c\\uddf3\", \"#Dharm_Nagari_ \\u0930\\u093e\\u0937\\u094d\\u091f\\u094d\\u0930 \\u0938\\u0930\\u094d\\u0935\\u094b\\u092a\\u0930\\u093f\", \"SHERLOCK\", \"NewsQeet\", \"Canada in India\", \"\\u0927\\u0930\\u092e \\u091c\\u0940 (\\u092e\\u093e\\u0928\\u0935)\", \"THE GROWING BUDS FOUNDATION\", \"Amit Verma\", \"Qor7 Technologies\", \"SpicyIP\", \"Bioinfoindia\", \"ISC - Indian School of Commerce\", \"gabbuprasad\", \"\\u092c\\u094d\\u0930\\u093e\\u092e\\u0923 \\u0915\\u093e \\u091b\\u094b\\u0930\\u093e\", \"Realising_Life\", \"Vinod Tiwari\", \"SEEDS India\", \"Salman Zafar\", \"Blood Donor Today\", \"SAFE2GO\\u2122\", \"Dassault Systemes IN\", \"Ms Socksy\", \"Ncsofttech : NCPC Softtech Private Limited\", \"30TH FEB\", \"Namasthe Telangana\", \"Pramod Vishwakarma\", \"Grand Slam Fitness\", \"Puneet Sharma | CodeRedMan\", \"Su-Rappa\", \"GOODTIMES\", \"Kuldeep Meena \\ud83d\\udd4a\", \"not important\", \"Sleepy Classes\", \"Jawahar\\ud83c\\uddee\\ud83c\\uddf3\", \"Preshit Shah\", \"Madhumay\", \"Itsduty\", \"Suhel Narvekar \\ud83c\\uddee\\ud83c\\uddf3 \\\\( \\u00f6 )/\", \"Yash Verulkar\", \"Advocate V. K. Jain\\ud83c\\uddee\\ud83c\\uddf3\", \"DP Daniel\", \"Ashish Ranjan Swain\", \"Mrunal\"], \"type\": \"bar\", \"x\": [\"Hindustan Times\", \"ANI\", \"Deccan Herald\", \"Business Standard\", \"NewsMobile\", \"The Hawk\", \"IndiaToday\", \"NDTV\", \"ORF\", \"Covid India Seva\"], \"y\": [280, 233, 136, 129, 128, 79, 78, 77, 62, 61]}],                        {\"autosize\": false, \"barmode\": \"group\", \"height\": 500, \"margin\": {\"b\": 100, \"l\": 50, \"pad\": 4, \"r\": 50, \"t\": 100}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 700},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('9729fa52-82ac-42f0-94b0-63db351498be');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googletrans\n\nfrom googletrans import Translator\n\ndef translate_hindi_to_english(hindi_tweet):\n    translator = Translator()\n    return translator.translate(hindi_tweet, dest=\"english\").text","execution_count":13,"outputs":[{"output_type":"stream","text":"Collecting googletrans\n  Downloading googletrans-3.0.0.tar.gz (17 kB)\nCollecting httpx==0.13.3\n  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 1.2 MB/s eta 0:00:011\n\u001b[?25hCollecting sniffio\n  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\nCollecting httpcore==0.9.*\n  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 894 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans) (2020.6.20)\nCollecting hstspreload\n  Downloading hstspreload-2020.10.20-py3-none-any.whl (972 kB)\n\u001b[K     |████████████████████████████████| 972 kB 4.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet==3.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans) (3.0.4)\nRequirement already satisfied: idna==2.* in /opt/conda/lib/python3.7/site-packages (from httpx==0.13.3->googletrans) (2.9)\nCollecting rfc3986<2,>=1.3\n  Downloading rfc3986-1.4.0-py2.py3-none-any.whl (31 kB)\nCollecting h11<0.10,>=0.8\n  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s  eta 0:00:01\n\u001b[?25hCollecting h2==3.*\n  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 2.7 MB/s  eta 0:00:01\n\u001b[?25hCollecting hpack<4,>=3.0\n  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nCollecting hyperframe<6,>=5.2.0\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15734 sha256=83e059c03ad8437b37df129b6c4c301c0d2d74b556d80d2106e694d707a29a17\n  Stored in directory: /root/.cache/pip/wheels/20/da/eb/a54579056f265eede0417df537dd56d3df5b9eb2b25df0003d\nSuccessfully built googletrans\nInstalling collected packages: sniffio, h11, hpack, hyperframe, h2, httpcore, hstspreload, rfc3986, httpx, googletrans\nSuccessfully installed googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.10.20 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.2.0\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install clean-text\n\nfrom cleantext import clean\n\ndef clean_hindi_tweets(hindi_tweet):\n    preprocessed_tweet = clean(original_hindi_tweet,\n    fix_unicode=True,               # fix various unicode errors\n    to_ascii=True,                  # transliterate to closest ASCII representation\n    lower=True,                     # lowercase text\n    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n    no_urls=True,                  # replace all URLs with a special token\n    no_emails=True,                # replace all email addresses with a special token\n    no_phone_numbers=True,         # replace all phone numbers with a special token\n    no_numbers=True,               # replace all numbers with a special token\n    no_digits=True,                # replace all digits with a special token\n    no_currency_symbols=True,      # replace all currency symbols with a special token\n    no_punct=True,                 # fully remove punctuation\n    replace_with_url=\"<URL>\",\n    replace_with_email=\"<EMAIL>\",\n    replace_with_phone_number=\"<PHONE>\",\n    replace_with_number=\"<NUMBER>\",\n    replace_with_digit=\"0\",\n    replace_with_currency_symbol=\"<CUR>\"\n                              )\n    return preprocessed_tweet","execution_count":14,"outputs":[{"output_type":"stream","text":"Collecting clean-text\n  Downloading clean_text-0.3.0-py3-none-any.whl (9.6 kB)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.7/site-packages (from clean-text) (0.6.0)\nCollecting ftfy<6.0,>=5.8\n  Downloading ftfy-5.8.tar.gz (64 kB)\n\u001b[K     |████████████████████████████████| 64 kB 992 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy<6.0,>=5.8->clean-text) (0.1.9)\nBuilding wheels for collected packages: ftfy\n  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45612 sha256=46593d54e134e4265570d239a0ec6b8771c21cf9b54739112f4056f366947fcc\n  Stored in directory: /root/.cache/pip/wheels/49/1c/fc/8b19700f939810cd8fd9495ae34934b246279791288eda1c31\nSuccessfully built ftfy\nInstalling collected packages: ftfy, clean-text\nSuccessfully installed clean-text-0.3.0 ftfy-5.8\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_pipeline(df):\n    translated_df = df.apply(lambda hindi_tweet: translate_hindi_to_english(hindi_tweet))\n    #clean = translated_df.apply(lambda english_tweet: clean_hindi_tweets(english_tweet))\n    return translated_df ","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_df.shape","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"(60, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"(eda_df['text'][0]).to_string()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"'0    अमेरिकी संस्था 3एम द्वारा कोरोना संकट से जूझने...\\n0    अमेरिकी संस्था 3एम द्वारा कोरोना संकट से जूझने...\\n0    #IndiaFightsCorona\\\\n\\\\nभूलें नहीं! अपने हाथ साब...\\n0    RT @RenuGulati18: फैसले की घड़ी है...............'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Translator().translate((eda_df['text'][0]).to_string(), dest=\"english\").text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:#eb9834;text-align:center\">Deep Learning Models</h1>"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\"> LSTM + No Word Embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_df = pd.read_csv('/kaggle/input/twitterdata/finalSentimentdata2.csv')\n\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english')[10:15])\n\ndef punctuation_stopwords_removal(sms):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in sms if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_sms = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_sms\n\nsentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(punctuation_stopwords_removal)\n\nreviews_split = []\nfor i, j in sentiment_df.iterrows():\n    reviews_split.append(j['text'])\n    \nwords = []\nfor review in reviews_split:\n    for word in review:\n        words.append(word)\n        \n        \nfrom collections import Counter\n\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n\nencoded_reviews = []\nfor review in reviews_split:\n    encoded_reviews.append([vocab_to_int[word] for word in review])\n    \n    \nlabels_to_int = []\nfor i, j in sentiment_df.iterrows():\n    if j['sentiment']=='joy':\n        labels_to_int.append(1)\n    else:\n        labels_to_int.append(0)\n        \n    \nreviews_len = Counter([len(x) for x in encoded_reviews])\nprint(max(reviews_len))\n\nnon_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\nencoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\nencoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n\ndef pad_features(reviews_int, seq_length):\n    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n    for i, row in enumerate(reviews_int):\n        if len(row)!=0:\n            features[i, -len(row):] = np.array(row)[:seq_length]\n    return features\n\nseq_length = 50\npadded_features= pad_features(encoded_reviews, seq_length)\nprint(padded_features[:2])\n\nsplit_frac = 0.8\nsplit_idx = int(len(padded_features)*split_frac)\n\ntraining_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\ntraining_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# torch.from_numpy creates a tensor data from n-d array\ntrain_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n\nbatch_size = 1\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)\n\ngpu_available = torch.cuda.is_available\n\nif gpu_available:\n    print('Training on GPU')\nelse:\n    print('GPU not available')","execution_count":18,"outputs":[{"output_type":"stream","text":"[\"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n48\n[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0  853  186   20 1079 1457 4429 2201  407 1240 1079\n    15  218  337  167  253  462  337  122  168 4430 4431  140   23  264\n    58  765    3    5  195 1079 2966  274]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0   80 1755 4432 2967\n  4433   86  854 1080 2968 4434 4435    7]]\nTraining on GPU\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass Basic_LSTM(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n        super(Basic_LSTM, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1 # either happy or sad\nhidden_dim = 256\nn_layers = 2\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_model_params(embedding_dim=None):\n    lr = 0.001\n    criterion = nn.BCELoss()\n    epochs = 4\n    count = 0\n    print_every = 100\n    clip = 5 \n    return [lr, criterion, epochs, count, print_every, clip, embedding_dim]","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_vanilla_models(basic_model):\n    if gpu_available:\n        basic_model.cuda()\n\n    lr, criterion, epochs, count, print_every, clip, embed_dim = initialize_model_params()\n    optimizer = torch.optim.Adam(basic_model.parameters(), lr=lr)\n    \n    basic_model.train()\n\n    for e in range(epochs):\n        # initialize lstm's hidden layer \n        h = basic_model.init_hidden(batch_size)\n        for inputs, labels in train_loader:\n            count += 1\n            if gpu_available:\n                inputs, labels = inputs.cuda(), labels.cuda()\n            h = tuple([each.data for each in h])\n\n            # training process\n            basic_model.zero_grad()\n            outputs, h = basic_model(inputs, h)\n            loss = criterion(outputs.squeeze(), labels.float())\n            loss.backward()\n            nn.utils.clip_grad_norm(basic_model.parameters(), clip)\n            optimizer.step()\n\n            # print average training losses\n            if count % print_every == 0:\n                val_h = basic_model.init_hidden(batch_size)\n                val_losses = []\n                basic_model.eval()\n                for inputs, labels in valid_loader:\n                    val_h = tuple([each.data for each in val_h])\n                    if gpu_available:\n                        inputs, labels = inputs.cuda(), labels.cuda()\n                outputs, val_h = basic_model(inputs, val_h)\n                val_loss = criterion(outputs.squeeze(), labels.float())\n                val_losses.append(val_loss.item())\n\n                basic_model.train()\n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(count),\n                      \"Loss: {:.6f}...\".format(loss.item()),\n                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n    return basic_model","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f03297;text-align:center\">Common Model functions</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import codecs \n\ndef return_embedding_matrix(embedding_path, embedding_dim, encoding=False):\n    embedding_dict={}\n    \n    if encoding:\n        f = codecs.open(embedding_path, encoding='utf-8')\n        for line in f:\n            values = line.rstrip().rsplit(' ')\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embedding_dict[word] = coefs\n        f.close()\n    else:\n        with codecs.open(embedding_path,'r') as f:\n            for line in f:\n                values=line.split()\n                word=values[0]\n                vectors=np.asarray(values[1:],'float32')\n                embedding_dict[word]=vectors\n        f.close()\n    \n    num_words=len(vocab_to_int)+1\n    \n    embedding_matrix=np.zeros((num_words,embedding_dim))\n\n    for word,i in (vocab_to_int.items()):\n        if i > num_words:\n            continue\n\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec\n    \n    return embedding_matrix","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_with_embeddings(net, embedding_dim):\n    lr, criterion, epochs, count, print_every, clip, embedding_dim = initialize_model_params(embedding_dim)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    if gpu_available:\n        net.cuda()\n\n    net.train()\n    for e in range(epochs):\n        # initialize lstm's hidden layer \n        h = net.init_hidden(batch_size)\n        for inputs, labels in train_loader:\n            count += 1\n            if gpu_available:\n                inputs, labels = inputs.cuda(), labels.cuda()\n            h = tuple([each.data for each in h])\n\n            # training process\n            net.zero_grad()\n            outputs, h = net(inputs, h)\n            loss = criterion(outputs.squeeze(), labels.float())\n            loss.backward()\n            nn.utils.clip_grad_norm(net.parameters(), clip)\n            optimizer.step()\n\n            # print average training losses\n            if count % print_every == 0:\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for inputs, labels in valid_loader:\n                    val_h = tuple([each.data for each in val_h])\n                    if gpu_available:\n                        inputs, labels = inputs.cuda(), labels.cuda()\n                outputs, val_h = net(inputs, val_h)\n                val_loss = criterion(outputs.squeeze(), labels.float())\n                val_losses.append(val_loss.item())\n\n                net.train()\n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(count),\n                      \"Loss: {:.6f}...\".format(loss.item()),\n                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n\n    return net","execution_count":58,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_model_accuracy(net, loader):\n    test_losses = []\n    num_correct = 0\n    lr, criterion, epochs, count, print_every, clip, embedding_dim = initialize_model_params()\n    h = net.init_hidden(batch_size)\n    net.eval()\n\n    for inputs, labels in loader:\n        h = tuple([each.data for each in h])\n        if gpu_available:\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        outputs, h = net(inputs, h)\n        test_loss = criterion(outputs.squeeze(), labels.float())\n        test_losses.append(test_loss.item())\n        pred = torch.round(outputs.squeeze())\n        correct_tensor = pred.eq(labels.float().view_as(pred))\n        correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n        num_correct += np.sum(correct)\n\n    # printing average statistics\n    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n    # accuracy over all test data\n    acc = num_correct/len(test_loader.dataset)\n    print(\"accuracy: {:.3f}\".format(acc))\n    return acc\n    ","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\">Basic LSTM + GloVe Embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\n\nclass BasicLSTMwithGloVeEmbeddings(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.2):\n        super(BasicLSTMwithGloVeEmbeddings, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding_layer.weight = nn.Parameter(embedding_matrix, requires_grad=False)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_embedding_matrix = return_embedding_matrix('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', 100)","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_lstm_with_glove_embeddings = BasicLSTMwithGloVeEmbeddings(vocab_size, output_size, 100, hidden_dim, n_layers, torch.Tensor(glove_embedding_matrix))\nprint(basic_lstm_with_glove_embeddings)","execution_count":82,"outputs":[{"output_type":"stream","text":"BasicLSTMwithGloVeEmbeddings(\n  (embedding_layer): Embedding(10663, 100)\n  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.2)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model = train_model_with_embeddings(basic_lstm_with_glove_embeddings, 100)","execution_count":83,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning:\n\ntorch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.105160... Val Loss: 0.101682\nEpoch: 1/4... Step: 200... Loss: 0.125974... Val Loss: 0.121315\nEpoch: 1/4... Step: 300... Loss: 0.091476... Val Loss: 0.126305\nEpoch: 1/4... Step: 400... Loss: 0.288006... Val Loss: 0.281274\nEpoch: 1/4... Step: 500... Loss: 1.488386... Val Loss: 0.261048\nEpoch: 1/4... Step: 600... Loss: 0.220156... Val Loss: 0.191762\nEpoch: 1/4... Step: 700... Loss: 1.717618... Val Loss: 0.253166\nEpoch: 1/4... Step: 800... Loss: 1.278027... Val Loss: 0.309397\nEpoch: 1/4... Step: 900... Loss: 0.832837... Val Loss: 0.588811\nEpoch: 1/4... Step: 1000... Loss: 0.500096... Val Loss: 0.400177\nEpoch: 1/4... Step: 1100... Loss: 0.082469... Val Loss: 0.092443\nEpoch: 1/4... Step: 1200... Loss: 2.047207... Val Loss: 0.168546\nEpoch: 1/4... Step: 1300... Loss: 0.161240... Val Loss: 0.106712\nEpoch: 1/4... Step: 1400... Loss: 0.660582... Val Loss: 0.113790\nEpoch: 1/4... Step: 1500... Loss: 2.823969... Val Loss: 0.135459\nEpoch: 1/4... Step: 1600... Loss: 0.347466... Val Loss: 0.225324\nEpoch: 1/4... Step: 1700... Loss: 1.060351... Val Loss: 0.172999\nEpoch: 1/4... Step: 1800... Loss: 0.018723... Val Loss: 0.039185\nEpoch: 1/4... Step: 1900... Loss: 0.005819... Val Loss: 0.016845\nEpoch: 1/4... Step: 2000... Loss: 0.053622... Val Loss: 0.023433\nEpoch: 1/4... Step: 2100... Loss: 0.167376... Val Loss: 0.149266\nEpoch: 1/4... Step: 2200... Loss: 0.051318... Val Loss: 0.078312\nEpoch: 1/4... Step: 2300... Loss: 0.542562... Val Loss: 0.035752\nEpoch: 1/4... Step: 2400... Loss: 0.011024... Val Loss: 0.044218\nEpoch: 2/4... Step: 2500... Loss: 0.568078... Val Loss: 0.030400\nEpoch: 2/4... Step: 2600... Loss: 0.021865... Val Loss: 0.037414\nEpoch: 2/4... Step: 2700... Loss: 0.608672... Val Loss: 0.082400\nEpoch: 2/4... Step: 2800... Loss: 0.137003... Val Loss: 0.019962\nEpoch: 2/4... Step: 2900... Loss: 0.027631... Val Loss: 0.249861\nEpoch: 2/4... Step: 3000... Loss: 0.005972... Val Loss: 0.018159\nEpoch: 2/4... Step: 3100... Loss: 0.516377... Val Loss: 0.057529\nEpoch: 2/4... Step: 3200... Loss: 0.025754... Val Loss: 0.043069\nEpoch: 2/4... Step: 3300... Loss: 0.252180... Val Loss: 0.177403\nEpoch: 2/4... Step: 3400... Loss: 2.014662... Val Loss: 0.046673\nEpoch: 2/4... Step: 3500... Loss: 1.112285... Val Loss: 0.054304\nEpoch: 2/4... Step: 3600... Loss: 0.327444... Val Loss: 0.042977\nEpoch: 2/4... Step: 3700... Loss: 0.020436... Val Loss: 0.021839\nEpoch: 2/4... Step: 3800... Loss: 0.035548... Val Loss: 0.072514\nEpoch: 2/4... Step: 3900... Loss: 0.024662... Val Loss: 0.106951\nEpoch: 2/4... Step: 4000... Loss: 1.413052... Val Loss: 0.017461\nEpoch: 2/4... Step: 4100... Loss: 2.888916... Val Loss: 0.030780\nEpoch: 2/4... Step: 4200... Loss: 0.027303... Val Loss: 0.084795\nEpoch: 2/4... Step: 4300... Loss: 0.048944... Val Loss: 0.011052\nEpoch: 2/4... Step: 4400... Loss: 0.005906... Val Loss: 0.025856\nEpoch: 2/4... Step: 4500... Loss: 0.046154... Val Loss: 0.044000\nEpoch: 2/4... Step: 4600... Loss: 0.144227... Val Loss: 0.065289\nEpoch: 2/4... Step: 4700... Loss: 0.013055... Val Loss: 0.068082\nEpoch: 2/4... Step: 4800... Loss: 0.138660... Val Loss: 0.043382\nEpoch: 2/4... Step: 4900... Loss: 0.014200... Val Loss: 0.014710\nEpoch: 3/4... Step: 5000... Loss: 0.012703... Val Loss: 0.013534\nEpoch: 3/4... Step: 5100... Loss: 0.034849... Val Loss: 0.015281\nEpoch: 3/4... Step: 5200... Loss: 0.006695... Val Loss: 0.021477\nEpoch: 3/4... Step: 5300... Loss: 0.018764... Val Loss: 0.035504\nEpoch: 3/4... Step: 5400... Loss: 0.005066... Val Loss: 0.016889\nEpoch: 3/4... Step: 5500... Loss: 1.245288... Val Loss: 0.061732\nEpoch: 3/4... Step: 5600... Loss: 0.242352... Val Loss: 0.010218\nEpoch: 3/4... Step: 5700... Loss: 0.065762... Val Loss: 0.025004\nEpoch: 3/4... Step: 5800... Loss: 0.079614... Val Loss: 0.072255\nEpoch: 3/4... Step: 5900... Loss: 0.012509... Val Loss: 0.016967\nEpoch: 3/4... Step: 6000... Loss: 0.087296... Val Loss: 0.046324\nEpoch: 3/4... Step: 6100... Loss: 0.010052... Val Loss: 0.020502\nEpoch: 3/4... Step: 6200... Loss: 0.011149... Val Loss: 0.014437\nEpoch: 3/4... Step: 6300... Loss: 0.067004... Val Loss: 0.051208\nEpoch: 3/4... Step: 6400... Loss: 0.571201... Val Loss: 0.108299\nEpoch: 3/4... Step: 6500... Loss: 0.155018... Val Loss: 0.019432\nEpoch: 3/4... Step: 6600... Loss: 0.010716... Val Loss: 0.019703\nEpoch: 3/4... Step: 6700... Loss: 0.094451... Val Loss: 0.066507\nEpoch: 3/4... Step: 6800... Loss: 0.265223... Val Loss: 0.016514\nEpoch: 3/4... Step: 6900... Loss: 0.007303... Val Loss: 0.017580\nEpoch: 3/4... Step: 7000... Loss: 0.150700... Val Loss: 0.037844\nEpoch: 3/4... Step: 7100... Loss: 0.021560... Val Loss: 0.037187\nEpoch: 3/4... Step: 7200... Loss: 0.207464... Val Loss: 0.060859\nEpoch: 3/4... Step: 7300... Loss: 0.017516... Val Loss: 0.017636\nEpoch: 3/4... Step: 7400... Loss: 0.035153... Val Loss: 0.017753\nEpoch: 4/4... Step: 7500... Loss: 0.113920... Val Loss: 0.003501\nEpoch: 4/4... Step: 7600... Loss: 0.014791... Val Loss: 0.005142\nEpoch: 4/4... Step: 7700... Loss: 2.899491... Val Loss: 0.004835\nEpoch: 4/4... Step: 7800... Loss: 0.180924... Val Loss: 0.005182\nEpoch: 4/4... Step: 7900... Loss: 0.237450... Val Loss: 0.005252\nEpoch: 4/4... Step: 8000... Loss: 0.137417... Val Loss: 0.005291\nEpoch: 4/4... Step: 8100... Loss: 0.174351... Val Loss: 0.005473\nEpoch: 4/4... Step: 8200... Loss: 0.298076... Val Loss: 0.005562\nEpoch: 4/4... Step: 8300... Loss: 0.348250... Val Loss: 0.005492\nEpoch: 4/4... Step: 8400... Loss: 0.158906... Val Loss: 0.005797\nEpoch: 4/4... Step: 8500... Loss: 0.173308... Val Loss: 0.005644\nEpoch: 4/4... Step: 8600... Loss: 1.864929... Val Loss: 0.005658\nEpoch: 4/4... Step: 8700... Loss: 0.095441... Val Loss: 0.005671\nEpoch: 4/4... Step: 8800... Loss: 0.138368... Val Loss: 0.005984\nEpoch: 4/4... Step: 8900... Loss: 0.380868... Val Loss: 0.005864\nEpoch: 4/4... Step: 9000... Loss: 2.083109... Val Loss: 0.005927\nEpoch: 4/4... Step: 9100... Loss: 0.127375... Val Loss: 0.005921\nEpoch: 4/4... Step: 9200... Loss: 2.240702... Val Loss: 0.005924\nEpoch: 4/4... Step: 9300... Loss: 0.087420... Val Loss: 0.006118\nEpoch: 4/4... Step: 9400... Loss: 0.122113... Val Loss: 0.006176\nEpoch: 4/4... Step: 9500... Loss: 0.183422... Val Loss: 0.006479\nEpoch: 4/4... Step: 9600... Loss: 0.207803... Val Loss: 0.006470\nEpoch: 4/4... Step: 9700... Loss: 2.542000... Val Loss: 0.006574\nEpoch: 4/4... Step: 9800... Loss: 0.322634... Val Loss: 0.006906\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(basic_lstm_with_glove_embeddings, test_loader)\nvalid_acc = predict_model_accuracy(basic_lstm_with_glove_embeddings, valid_loader)\n\nprint('Basic lstm model with glove embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":84,"outputs":[{"output_type":"stream","text":"Test loss: 0.356\naccuracy: 0.890\nTest loss: 0.279\naccuracy: 0.909\nBasic lstm model with glove embeddings\nTest accuracy :  0.889967637540453\nvalid accuracy :  0.9093851132686084\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\"> LSTM + FastText Embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\n\nclass BasicLSTMwithFastTextEmbeddings(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.2):\n        super(BasicLSTMwithFastTextEmbeddings, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding_layer.weight = nn.Parameter(embedding_matrix, requires_grad=False)\n        \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fasttext_embedding_matrix = return_embedding_matrix('/kaggle/input/fasttext/wiki.simple.vec', 300, True)","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_lstm_with_fasttext_embeddings = BasicLSTMwithFastTextEmbeddings(vocab_size, output_size, 300, hidden_dim, n_layers, torch.Tensor(fasttext_embedding_matrix))\nprint(basic_lstm_with_fasttext_embeddings)","execution_count":86,"outputs":[{"output_type":"stream","text":"BasicLSTMwithFastTextEmbeddings(\n  (embedding_layer): Embedding(10663, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model_2 = train_model_with_embeddings(basic_lstm_with_fasttext_embeddings, 300)","execution_count":87,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning:\n\ntorch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.086626... Val Loss: 0.087720\nEpoch: 1/4... Step: 200... Loss: 0.083929... Val Loss: 0.096534\nEpoch: 1/4... Step: 300... Loss: 0.108860... Val Loss: 0.114931\nEpoch: 1/4... Step: 400... Loss: 0.189821... Val Loss: 0.244313\nEpoch: 1/4... Step: 500... Loss: 1.455628... Val Loss: 0.229327\nEpoch: 1/4... Step: 600... Loss: 0.256467... Val Loss: 0.168795\nEpoch: 1/4... Step: 700... Loss: 1.936853... Val Loss: 0.191222\nEpoch: 1/4... Step: 800... Loss: 1.607762... Val Loss: 0.246777\nEpoch: 1/4... Step: 900... Loss: 0.758041... Val Loss: 0.512449\nEpoch: 1/4... Step: 1000... Loss: 0.155542... Val Loss: 0.192833\nEpoch: 1/4... Step: 1100... Loss: 0.152072... Val Loss: 0.197456\nEpoch: 1/4... Step: 1200... Loss: 1.824702... Val Loss: 0.277847\nEpoch: 1/4... Step: 1300... Loss: 0.156877... Val Loss: 0.132162\nEpoch: 1/4... Step: 1400... Loss: 0.040726... Val Loss: 0.135543\nEpoch: 1/4... Step: 1500... Loss: 2.183458... Val Loss: 0.178617\nEpoch: 1/4... Step: 1600... Loss: 0.503541... Val Loss: 0.284909\nEpoch: 1/4... Step: 1700... Loss: 2.165854... Val Loss: 0.210519\nEpoch: 1/4... Step: 1800... Loss: 0.019507... Val Loss: 0.069284\nEpoch: 1/4... Step: 1900... Loss: 0.029057... Val Loss: 0.098570\nEpoch: 1/4... Step: 2000... Loss: 0.393116... Val Loss: 0.078645\nEpoch: 1/4... Step: 2100... Loss: 0.085792... Val Loss: 0.198084\nEpoch: 1/4... Step: 2200... Loss: 0.072732... Val Loss: 0.170402\nEpoch: 1/4... Step: 2300... Loss: 0.607872... Val Loss: 0.068701\nEpoch: 1/4... Step: 2400... Loss: 0.034799... Val Loss: 0.193590\nEpoch: 2/4... Step: 2500... Loss: 0.555346... Val Loss: 0.039684\nEpoch: 2/4... Step: 2600... Loss: 0.044177... Val Loss: 0.133904\nEpoch: 2/4... Step: 2700... Loss: 0.623097... Val Loss: 0.229657\nEpoch: 2/4... Step: 2800... Loss: 0.882420... Val Loss: 0.015694\nEpoch: 2/4... Step: 2900... Loss: 0.021967... Val Loss: 0.087141\nEpoch: 2/4... Step: 3000... Loss: 0.155422... Val Loss: 0.194351\nEpoch: 2/4... Step: 3100... Loss: 0.652896... Val Loss: 0.183645\nEpoch: 2/4... Step: 3200... Loss: 0.138840... Val Loss: 0.321010\nEpoch: 2/4... Step: 3300... Loss: 0.148376... Val Loss: 0.186027\nEpoch: 2/4... Step: 3400... Loss: 3.029256... Val Loss: 0.096823\nEpoch: 2/4... Step: 3500... Loss: 0.110940... Val Loss: 0.253870\nEpoch: 2/4... Step: 3600... Loss: 2.666170... Val Loss: 0.193397\nEpoch: 2/4... Step: 3700... Loss: 0.055379... Val Loss: 0.113468\nEpoch: 2/4... Step: 3800... Loss: 0.121496... Val Loss: 0.429294\nEpoch: 2/4... Step: 3900... Loss: 0.037598... Val Loss: 0.080475\nEpoch: 2/4... Step: 4000... Loss: 1.449826... Val Loss: 0.229524\nEpoch: 2/4... Step: 4100... Loss: 0.377554... Val Loss: 0.475686\nEpoch: 2/4... Step: 4200... Loss: 0.027176... Val Loss: 0.229988\nEpoch: 2/4... Step: 4300... Loss: 0.207948... Val Loss: 0.041770\nEpoch: 2/4... Step: 4400... Loss: 0.018956... Val Loss: 0.154990\nEpoch: 2/4... Step: 4500... Loss: 0.029650... Val Loss: 0.209475\nEpoch: 2/4... Step: 4600... Loss: 0.666178... Val Loss: 0.195871\nEpoch: 2/4... Step: 4700... Loss: 0.031539... Val Loss: 0.235917\nEpoch: 2/4... Step: 4800... Loss: 0.181779... Val Loss: 0.264492\nEpoch: 2/4... Step: 4900... Loss: 0.019082... Val Loss: 0.030048\nEpoch: 3/4... Step: 5000... Loss: 0.006789... Val Loss: 0.023509\nEpoch: 3/4... Step: 5100... Loss: 0.326447... Val Loss: 0.082101\nEpoch: 3/4... Step: 5200... Loss: 0.013142... Val Loss: 0.087832\nEpoch: 3/4... Step: 5300... Loss: 0.038808... Val Loss: 0.514944\nEpoch: 3/4... Step: 5400... Loss: 0.019024... Val Loss: 0.025998\nEpoch: 3/4... Step: 5500... Loss: 1.396580... Val Loss: 0.068870\nEpoch: 3/4... Step: 5600... Loss: 0.217575... Val Loss: 0.092538\nEpoch: 3/4... Step: 5700... Loss: 0.073768... Val Loss: 0.113023\nEpoch: 3/4... Step: 5800... Loss: 0.142044... Val Loss: 0.090988\nEpoch: 3/4... Step: 5900... Loss: 0.064995... Val Loss: 0.076114\nEpoch: 3/4... Step: 6000... Loss: 0.554501... Val Loss: 0.111623\nEpoch: 3/4... Step: 6100... Loss: 0.263795... Val Loss: 0.286468\nEpoch: 3/4... Step: 6200... Loss: 0.034121... Val Loss: 0.138585\nEpoch: 3/4... Step: 6300... Loss: 0.386552... Val Loss: 0.120475\nEpoch: 3/4... Step: 6400... Loss: 0.540884... Val Loss: 0.205346\nEpoch: 3/4... Step: 6500... Loss: 1.105250... Val Loss: 0.093431\nEpoch: 3/4... Step: 6600... Loss: 0.019949... Val Loss: 0.223389\nEpoch: 3/4... Step: 6700... Loss: 0.104115... Val Loss: 0.066696\nEpoch: 3/4... Step: 6800... Loss: 0.106355... Val Loss: 0.027330\nEpoch: 3/4... Step: 6900... Loss: 0.008368... Val Loss: 0.112316\nEpoch: 3/4... Step: 7000... Loss: 0.041604... Val Loss: 0.107703\nEpoch: 3/4... Step: 7100... Loss: 0.013115... Val Loss: 0.055784\nEpoch: 3/4... Step: 7200... Loss: 0.100474... Val Loss: 0.131051\nEpoch: 3/4... Step: 7300... Loss: 0.020058... Val Loss: 0.029873\nEpoch: 3/4... Step: 7400... Loss: 0.152216... Val Loss: 0.037597\nEpoch: 4/4... Step: 7500... Loss: 0.053925... Val Loss: 0.016884\nEpoch: 4/4... Step: 7600... Loss: 0.002599... Val Loss: 0.008909\nEpoch: 4/4... Step: 7700... Loss: 0.110875... Val Loss: 0.022413\nEpoch: 4/4... Step: 7800... Loss: 0.001699... Val Loss: 0.005069\nEpoch: 4/4... Step: 7900... Loss: 0.008866... Val Loss: 0.039228\nEpoch: 4/4... Step: 8000... Loss: 0.013613... Val Loss: 0.066716\nEpoch: 4/4... Step: 8100... Loss: 0.002953... Val Loss: 0.026952\nEpoch: 4/4... Step: 8200... Loss: 1.965609... Val Loss: 0.342604\nEpoch: 4/4... Step: 8300... Loss: 0.015106... Val Loss: 0.041506\nEpoch: 4/4... Step: 8400... Loss: 0.006357... Val Loss: 0.035077\nEpoch: 4/4... Step: 8500... Loss: 0.218960... Val Loss: 0.116317\nEpoch: 4/4... Step: 8600... Loss: 0.151725... Val Loss: 0.104072\nEpoch: 4/4... Step: 8700... Loss: 0.005283... Val Loss: 0.019432\nEpoch: 4/4... Step: 8800... Loss: 0.068539... Val Loss: 0.018066\nEpoch: 4/4... Step: 8900... Loss: 0.008324... Val Loss: 0.020792\nEpoch: 4/4... Step: 9000... Loss: 0.075565... Val Loss: 0.195116\nEpoch: 4/4... Step: 9100... Loss: 1.027280... Val Loss: 0.053059\nEpoch: 4/4... Step: 9200... Loss: 0.080473... Val Loss: 0.041252\nEpoch: 4/4... Step: 9300... Loss: 0.188768... Val Loss: 0.055118\nEpoch: 4/4... Step: 9400... Loss: 0.047702... Val Loss: 0.073587\nEpoch: 4/4... Step: 9500... Loss: 0.286816... Val Loss: 0.069959\nEpoch: 4/4... Step: 9600... Loss: 0.011210... Val Loss: 0.027324\nEpoch: 4/4... Step: 9700... Loss: 0.097980... Val Loss: 0.020474\nEpoch: 4/4... Step: 9800... Loss: 0.001501... Val Loss: 0.004560\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(trained_model_2, test_loader)\nvalid_acc = predict_model_accuracy(trained_model_2, valid_loader)\n\nprint('Basic lstm model with fasttext embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":88,"outputs":[{"output_type":"stream","text":"Test loss: 0.235\naccuracy: 0.926\nTest loss: 0.349\naccuracy: 0.890\nBasic lstm model with glove embeddings\nTest accuracy :  0.9255663430420712\nvalid accuracy :  0.889967637540453\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\"> LSTM + Crisis Embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\n\nclass BasicLSTMwithCrisisEmbeddings(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.2):\n        super(BasicLSTMwithCrisisEmbeddings, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding_layer.weight = nn.Parameter(embedding_matrix, requires_grad=False)\n        \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crisis_embedding_matrix = return_embedding_matrix('/kaggle/input/crisis-data/crisis_embeddings.text', 300)","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crisis_model = BasicLSTMwithCrisisEmbeddings(vocab_size, output_size, 300, hidden_dim, n_layers, torch.Tensor(crisis_embedding_matrix))\nprint(crisis_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model_3 = train_model_with_embeddings(crisis_model, 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(trained_model_3, test_loader)\nvalid_acc = predict_model_accuracy(trained_model_3, valid_loader)\n\nprint('Basic lstm model with crisis embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\">Bi-LSTM + no word embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass Basic_BiLSTM(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2, embedding_matrix=None):\n        super(Basic_BiLSTM, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"basic_bilstm = Basic_BiLSTM(vocab_size, output_size, 300, hidden_dim, n_layers)\nprint(basic_bilstm)","execution_count":41,"outputs":[{"output_type":"stream","text":"Basic_BiLSTM(\n  (embedding_layer): Embedding(10663, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"trained_model_4 = train_vanilla_models(basic_bilstm)","execution_count":48,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning:\n\ntorch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.017282... Val Loss: 0.096784\nEpoch: 1/4... Step: 200... Loss: 0.113922... Val Loss: 0.138673\nEpoch: 1/4... Step: 300... Loss: 0.074475... Val Loss: 0.115637\nEpoch: 1/4... Step: 400... Loss: 0.144455... Val Loss: 0.215506\nEpoch: 1/4... Step: 500... Loss: 0.990616... Val Loss: 0.570731\nEpoch: 1/4... Step: 600... Loss: 0.148018... Val Loss: 0.933563\nEpoch: 1/4... Step: 700... Loss: 1.026199... Val Loss: 0.090294\nEpoch: 1/4... Step: 800... Loss: 0.920068... Val Loss: 0.081625\nEpoch: 1/4... Step: 900... Loss: 0.891357... Val Loss: 0.921803\nEpoch: 1/4... Step: 1000... Loss: 0.070162... Val Loss: 0.149202\nEpoch: 1/4... Step: 1100... Loss: 0.094482... Val Loss: 0.789988\nEpoch: 1/4... Step: 1200... Loss: 1.066088... Val Loss: 0.741417\nEpoch: 1/4... Step: 1300... Loss: 0.022373... Val Loss: 0.034160\nEpoch: 1/4... Step: 1400... Loss: 0.088019... Val Loss: 0.095168\nEpoch: 1/4... Step: 1500... Loss: 1.975881... Val Loss: 0.070693\nEpoch: 1/4... Step: 1600... Loss: 0.153369... Val Loss: 0.781061\nEpoch: 1/4... Step: 1700... Loss: 2.587835... Val Loss: 0.751121\nEpoch: 1/4... Step: 1800... Loss: 0.049521... Val Loss: 0.589272\nEpoch: 1/4... Step: 1900... Loss: 0.037088... Val Loss: 0.333470\nEpoch: 1/4... Step: 2000... Loss: 0.109196... Val Loss: 0.031729\nEpoch: 1/4... Step: 2100... Loss: 0.326369... Val Loss: 0.580882\nEpoch: 1/4... Step: 2200... Loss: 0.034401... Val Loss: 0.038137\nEpoch: 1/4... Step: 2300... Loss: 0.797188... Val Loss: 0.038844\nEpoch: 1/4... Step: 2400... Loss: 0.058087... Val Loss: 0.421827\nEpoch: 2/4... Step: 2500... Loss: 0.573221... Val Loss: 0.033787\nEpoch: 2/4... Step: 2600... Loss: 0.012705... Val Loss: 0.043374\nEpoch: 2/4... Step: 2700... Loss: 0.488245... Val Loss: 0.039856\nEpoch: 2/4... Step: 2800... Loss: 0.166215... Val Loss: 0.017449\nEpoch: 2/4... Step: 2900... Loss: 0.005269... Val Loss: 0.046043\nEpoch: 2/4... Step: 3000... Loss: 0.006983... Val Loss: 0.086456\nEpoch: 2/4... Step: 3100... Loss: 0.493651... Val Loss: 0.466479\nEpoch: 2/4... Step: 3200... Loss: 0.008825... Val Loss: 0.034979\nEpoch: 2/4... Step: 3300... Loss: 0.077585... Val Loss: 0.037750\nEpoch: 2/4... Step: 3400... Loss: 0.034604... Val Loss: 0.752875\nEpoch: 2/4... Step: 3500... Loss: 0.255047... Val Loss: 0.013409\nEpoch: 2/4... Step: 3600... Loss: 0.019784... Val Loss: 0.040865\nEpoch: 2/4... Step: 3700... Loss: 0.031381... Val Loss: 0.043406\nEpoch: 2/4... Step: 3800... Loss: 0.020586... Val Loss: 0.219860\nEpoch: 2/4... Step: 3900... Loss: 0.007972... Val Loss: 0.300621\nEpoch: 2/4... Step: 4000... Loss: 0.034776... Val Loss: 0.195907\nEpoch: 2/4... Step: 4100... Loss: 0.142958... Val Loss: 0.020489\nEpoch: 2/4... Step: 4200... Loss: 0.009237... Val Loss: 0.025917\nEpoch: 2/4... Step: 4300... Loss: 2.059381... Val Loss: 0.014434\nEpoch: 2/4... Step: 4400... Loss: 0.002486... Val Loss: 0.267088\nEpoch: 2/4... Step: 4500... Loss: 0.039252... Val Loss: 0.075208\nEpoch: 2/4... Step: 4600... Loss: 0.075094... Val Loss: 0.087372\nEpoch: 2/4... Step: 4700... Loss: 0.003245... Val Loss: 0.544361\nEpoch: 2/4... Step: 4800... Loss: 0.175877... Val Loss: 0.398259\nEpoch: 2/4... Step: 4900... Loss: 0.004603... Val Loss: 1.234664\nEpoch: 3/4... Step: 5000... Loss: 0.002094... Val Loss: 0.144183\nEpoch: 3/4... Step: 5100... Loss: 0.004439... Val Loss: 0.063127\nEpoch: 3/4... Step: 5200... Loss: 0.000591... Val Loss: 0.015518\nEpoch: 3/4... Step: 5300... Loss: 0.000994... Val Loss: 1.271879\nEpoch: 3/4... Step: 5400... Loss: 0.000483... Val Loss: 0.411862\nEpoch: 3/4... Step: 5500... Loss: 0.006350... Val Loss: 0.131645\nEpoch: 3/4... Step: 5600... Loss: 0.012356... Val Loss: 0.050005\nEpoch: 3/4... Step: 5700... Loss: 0.000957... Val Loss: 0.003045\nEpoch: 3/4... Step: 5800... Loss: 0.000337... Val Loss: 0.004713\nEpoch: 3/4... Step: 5900... Loss: 0.001402... Val Loss: 0.266215\nEpoch: 3/4... Step: 6000... Loss: 0.015348... Val Loss: 0.936256\nEpoch: 3/4... Step: 6100... Loss: 0.000826... Val Loss: 0.226409\nEpoch: 3/4... Step: 6200... Loss: 0.001037... Val Loss: 0.005261\nEpoch: 3/4... Step: 6300... Loss: 0.009199... Val Loss: 1.895631\nEpoch: 3/4... Step: 6400... Loss: 0.005109... Val Loss: 2.409131\nEpoch: 3/4... Step: 6500... Loss: 0.007866... Val Loss: 1.458072\nEpoch: 3/4... Step: 6600... Loss: 0.000843... Val Loss: 0.811957\nEpoch: 3/4... Step: 6700... Loss: 0.020034... Val Loss: 1.990088\nEpoch: 3/4... Step: 6800... Loss: 0.006715... Val Loss: 0.041922\nEpoch: 3/4... Step: 6900... Loss: 0.000698... Val Loss: 1.944550\nEpoch: 3/4... Step: 7000... Loss: 0.001737... Val Loss: 0.076340\nEpoch: 3/4... Step: 7100... Loss: 0.000491... Val Loss: 0.013243\nEpoch: 3/4... Step: 7200... Loss: 0.001716... Val Loss: 0.002820\nEpoch: 3/4... Step: 7300... Loss: 0.001154... Val Loss: 0.052665\nEpoch: 3/4... Step: 7400... Loss: 4.022805... Val Loss: 0.147322\nEpoch: 4/4... Step: 7500... Loss: 0.000689... Val Loss: 0.008053\nEpoch: 4/4... Step: 7600... Loss: 0.000320... Val Loss: 0.003807\nEpoch: 4/4... Step: 7700... Loss: 0.004294... Val Loss: 0.104595\nEpoch: 4/4... Step: 7800... Loss: 0.000913... Val Loss: 0.010688\nEpoch: 4/4... Step: 7900... Loss: 0.000159... Val Loss: 1.621444\nEpoch: 4/4... Step: 8000... Loss: 0.000076... Val Loss: 0.061004\nEpoch: 4/4... Step: 8100... Loss: 0.000088... Val Loss: 0.028749\nEpoch: 4/4... Step: 8200... Loss: 0.433478... Val Loss: 0.011589\nEpoch: 4/4... Step: 8300... Loss: 0.000803... Val Loss: 0.001968\nEpoch: 4/4... Step: 8400... Loss: 0.000224... Val Loss: 0.002042\nEpoch: 4/4... Step: 8500... Loss: 0.000078... Val Loss: 0.511104\nEpoch: 4/4... Step: 8600... Loss: 0.000512... Val Loss: 0.012797\nEpoch: 4/4... Step: 8700... Loss: 0.000033... Val Loss: 0.207592\nEpoch: 4/4... Step: 8800... Loss: 0.000216... Val Loss: 0.186872\nEpoch: 4/4... Step: 8900... Loss: 0.000030... Val Loss: 0.147613\nEpoch: 4/4... Step: 9000... Loss: 0.001481... Val Loss: 0.164991\nEpoch: 4/4... Step: 9100... Loss: 0.000554... Val Loss: 0.132391\nEpoch: 4/4... Step: 9200... Loss: 0.006685... Val Loss: 0.134549\nEpoch: 4/4... Step: 9300... Loss: 0.004794... Val Loss: 1.444595\nEpoch: 4/4... Step: 9400... Loss: 0.000628... Val Loss: 0.096159\nEpoch: 4/4... Step: 9500... Loss: 0.000243... Val Loss: 0.014704\nEpoch: 4/4... Step: 9600... Loss: 0.000232... Val Loss: 0.001355\nEpoch: 4/4... Step: 9700... Loss: 0.003445... Val Loss: 0.000810\nEpoch: 4/4... Step: 9800... Loss: 0.000758... Val Loss: 0.000713\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(trained_model_4, test_loader)\nvalid_acc = predict_model_accuracy(trained_model_4, valid_loader)\n\nprint('Basic bi-lstm model with no embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":55,"outputs":[{"output_type":"stream","text":"Test loss: 0.512\naccuracy: 0.874\nTest loss: 0.688\naccuracy: 0.861\nBasic bi-lstm model with no embeddings\nTest accuracy :  0.8737864077669902\nvalid accuracy :  0.86084142394822\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\">Bi-LSTM + GloVe embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass Basic_BiLSTM_with_embeddings(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.2):\n        super(Basic_BiLSTM_with_embeddings, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding_layer.weight = nn.Parameter(embedding_matrix, requires_grad=False)\n            \n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"basic_bilstm_with_glove_embeddings = Basic_BiLSTM_with_embeddings(vocab_size, output_size, 100, hidden_dim, n_layers, torch.Tensor(glove_embedding_matrix))\nprint(basic_bilstm_with_glove_embeddings)","execution_count":66,"outputs":[{"output_type":"stream","text":"Basic_BiLSTM_with_embeddings(\n  (embedding_layer): Embedding(10663, 100)\n  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"trained_model_5 = train_model_with_embeddings(basic_bilstm_with_glove_embeddings, 100)","execution_count":67,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning:\n\ntorch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.092845... Val Loss: 0.222516\nEpoch: 1/4... Step: 200... Loss: 0.108394... Val Loss: 0.265996\nEpoch: 1/4... Step: 300... Loss: 0.182916... Val Loss: 0.309097\nEpoch: 1/4... Step: 400... Loss: 0.296928... Val Loss: 0.432396\nEpoch: 1/4... Step: 500... Loss: 1.192190... Val Loss: 0.487297\nEpoch: 1/4... Step: 600... Loss: 0.342216... Val Loss: 0.505500\nEpoch: 1/4... Step: 700... Loss: 3.475111... Val Loss: 0.572548\nEpoch: 1/4... Step: 800... Loss: 1.009291... Val Loss: 0.549700\nEpoch: 1/4... Step: 900... Loss: 0.775130... Val Loss: 0.680100\nEpoch: 1/4... Step: 1000... Loss: 0.201934... Val Loss: 0.450449\nEpoch: 1/4... Step: 1100... Loss: 0.035510... Val Loss: 0.500708\nEpoch: 1/4... Step: 1200... Loss: 1.206396... Val Loss: 0.395102\nEpoch: 1/4... Step: 1300... Loss: 0.197514... Val Loss: 0.311014\nEpoch: 1/4... Step: 1400... Loss: 0.125393... Val Loss: 0.234874\nEpoch: 1/4... Step: 1500... Loss: 1.764850... Val Loss: 0.270800\nEpoch: 1/4... Step: 1600... Loss: 0.177257... Val Loss: 0.320892\nEpoch: 1/4... Step: 1700... Loss: 1.656578... Val Loss: 0.485063\nEpoch: 1/4... Step: 1800... Loss: 0.013674... Val Loss: 0.495779\nEpoch: 1/4... Step: 1900... Loss: 0.017259... Val Loss: 0.367778\nEpoch: 1/4... Step: 2000... Loss: 0.386513... Val Loss: 0.272332\nEpoch: 1/4... Step: 2100... Loss: 0.300086... Val Loss: 0.357358\nEpoch: 1/4... Step: 2200... Loss: 0.336404... Val Loss: 0.346265\nEpoch: 1/4... Step: 2300... Loss: 0.670867... Val Loss: 0.371792\nEpoch: 1/4... Step: 2400... Loss: 0.054270... Val Loss: 0.261949\nEpoch: 2/4... Step: 2500... Loss: 0.720709... Val Loss: 0.160619\nEpoch: 2/4... Step: 2600... Loss: 0.021443... Val Loss: 0.155749\nEpoch: 2/4... Step: 2700... Loss: 0.791979... Val Loss: 0.175942\nEpoch: 2/4... Step: 2800... Loss: 0.527284... Val Loss: 0.084613\nEpoch: 2/4... Step: 2900... Loss: 0.008927... Val Loss: 0.145117\nEpoch: 2/4... Step: 3000... Loss: 0.001661... Val Loss: 0.111156\nEpoch: 2/4... Step: 3100... Loss: 0.731967... Val Loss: 0.069022\nEpoch: 2/4... Step: 3200... Loss: 0.030597... Val Loss: 0.108019\nEpoch: 2/4... Step: 3300... Loss: 0.378609... Val Loss: 0.260168\nEpoch: 2/4... Step: 3400... Loss: 3.064671... Val Loss: 0.125399\nEpoch: 2/4... Step: 3500... Loss: 0.953018... Val Loss: 0.095315\nEpoch: 2/4... Step: 3600... Loss: 0.966451... Val Loss: 0.078387\nEpoch: 2/4... Step: 3700... Loss: 0.037132... Val Loss: 0.070116\nEpoch: 2/4... Step: 3800... Loss: 0.050670... Val Loss: 0.144325\nEpoch: 2/4... Step: 3900... Loss: 0.025304... Val Loss: 0.165879\nEpoch: 2/4... Step: 4000... Loss: 1.471489... Val Loss: 0.059892\nEpoch: 2/4... Step: 4100... Loss: 2.070569... Val Loss: 0.105496\nEpoch: 2/4... Step: 4200... Loss: 0.012004... Val Loss: 0.128973\nEpoch: 2/4... Step: 4300... Loss: 0.040968... Val Loss: 0.045392\nEpoch: 2/4... Step: 4400... Loss: 0.007529... Val Loss: 0.076700\nEpoch: 2/4... Step: 4500... Loss: 0.013124... Val Loss: 0.065120\nEpoch: 2/4... Step: 4600... Loss: 0.346178... Val Loss: 0.103618\nEpoch: 2/4... Step: 4700... Loss: 0.010088... Val Loss: 0.091151\nEpoch: 2/4... Step: 4800... Loss: 0.177715... Val Loss: 0.064591\nEpoch: 2/4... Step: 4900... Loss: 0.015471... Val Loss: 0.067715\nEpoch: 3/4... Step: 5000... Loss: 0.009262... Val Loss: 0.044667\nEpoch: 3/4... Step: 5100... Loss: 0.058635... Val Loss: 0.044952\nEpoch: 3/4... Step: 5200... Loss: 0.017658... Val Loss: 0.048257\nEpoch: 3/4... Step: 5300... Loss: 0.008807... Val Loss: 0.038381\nEpoch: 3/4... Step: 5400... Loss: 0.007444... Val Loss: 0.047264\nEpoch: 3/4... Step: 5500... Loss: 2.335934... Val Loss: 0.058212\nEpoch: 3/4... Step: 5600... Loss: 0.177983... Val Loss: 0.033493\nEpoch: 3/4... Step: 5700... Loss: 0.115207... Val Loss: 0.051066\nEpoch: 3/4... Step: 5800... Loss: 0.033116... Val Loss: 0.066914\nEpoch: 3/4... Step: 5900... Loss: 0.007490... Val Loss: 0.037745\nEpoch: 3/4... Step: 6000... Loss: 0.166744... Val Loss: 0.051547\nEpoch: 3/4... Step: 6100... Loss: 0.034158... Val Loss: 0.063682\nEpoch: 3/4... Step: 6200... Loss: 0.017541... Val Loss: 0.042253\nEpoch: 3/4... Step: 6300... Loss: 0.018659... Val Loss: 0.034563\nEpoch: 3/4... Step: 6400... Loss: 0.289209... Val Loss: 0.060845\nEpoch: 3/4... Step: 6500... Loss: 0.151959... Val Loss: 0.047811\nEpoch: 3/4... Step: 6600... Loss: 0.020295... Val Loss: 0.054088\nEpoch: 3/4... Step: 6700... Loss: 0.075003... Val Loss: 0.056806\nEpoch: 3/4... Step: 6800... Loss: 0.540818... Val Loss: 0.028363\nEpoch: 3/4... Step: 6900... Loss: 0.011809... Val Loss: 0.050514\nEpoch: 3/4... Step: 7000... Loss: 1.478419... Val Loss: 0.045664\nEpoch: 3/4... Step: 7100... Loss: 0.018160... Val Loss: 0.037044\nEpoch: 3/4... Step: 7200... Loss: 0.088785... Val Loss: 0.039488\nEpoch: 3/4... Step: 7300... Loss: 0.100693... Val Loss: 0.028285\nEpoch: 3/4... Step: 7400... Loss: 0.051913... Val Loss: 0.040901\nEpoch: 4/4... Step: 7500... Loss: 0.218351... Val Loss: 0.019788\nEpoch: 4/4... Step: 7600... Loss: 0.005202... Val Loss: 0.022325\nEpoch: 4/4... Step: 7700... Loss: 0.136111... Val Loss: 0.019994\nEpoch: 4/4... Step: 7800... Loss: 0.005901... Val Loss: 0.013999\nEpoch: 4/4... Step: 7900... Loss: 0.080411... Val Loss: 0.034991\nEpoch: 4/4... Step: 8000... Loss: 0.011667... Val Loss: 0.032929\nEpoch: 4/4... Step: 8100... Loss: 0.004752... Val Loss: 0.027221\nEpoch: 4/4... Step: 8200... Loss: 0.112413... Val Loss: 0.037523\nEpoch: 4/4... Step: 8300... Loss: 0.029650... Val Loss: 0.042803\nEpoch: 4/4... Step: 8400... Loss: 0.008058... Val Loss: 0.016276\nEpoch: 4/4... Step: 8500... Loss: 0.017542... Val Loss: 0.030253\nEpoch: 4/4... Step: 8600... Loss: 1.207582... Val Loss: 0.028035\nEpoch: 4/4... Step: 8700... Loss: 0.014012... Val Loss: 0.025052\nEpoch: 4/4... Step: 8800... Loss: 0.088510... Val Loss: 0.016543\nEpoch: 4/4... Step: 8900... Loss: 0.026663... Val Loss: 0.038900\nEpoch: 4/4... Step: 9000... Loss: 0.093998... Val Loss: 0.059098\nEpoch: 4/4... Step: 9100... Loss: 0.042158... Val Loss: 0.049424\nEpoch: 4/4... Step: 9200... Loss: 0.032789... Val Loss: 0.026789\nEpoch: 4/4... Step: 9300... Loss: 0.078121... Val Loss: 0.021307\nEpoch: 4/4... Step: 9400... Loss: 0.029022... Val Loss: 0.024458\nEpoch: 4/4... Step: 9500... Loss: 0.049833... Val Loss: 0.023759\nEpoch: 4/4... Step: 9600... Loss: 0.003907... Val Loss: 0.011478\nEpoch: 4/4... Step: 9700... Loss: 0.106732... Val Loss: 0.016658\nEpoch: 4/4... Step: 9800... Loss: 0.006740... Val Loss: 0.015171\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(trained_model_5, test_loader)\nvalid_acc = predict_model_accuracy(trained_model_5, valid_loader)\n\nprint('Basic bi-lstm model with GloVe embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":68,"outputs":[{"output_type":"stream","text":"Test loss: 0.254\naccuracy: 0.913\nTest loss: 0.259\naccuracy: 0.906\nBasic bi-lstm model with GloVe embeddings\nTest accuracy :  0.912621359223301\nvalid accuracy :  0.9061488673139159\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\">Bi-LSTM + FastText embeddings</h2>"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"basic_bilstm_with_fasttext_embeddings = Basic_BiLSTM_with_embeddings(vocab_size, output_size, 300, hidden_dim, n_layers, torch.Tensor(fasttext_embedding_matrix))\nprint(basic_bilstm_with_fasttext_embeddings)","execution_count":73,"outputs":[{"output_type":"stream","text":"Basic_BiLSTM_with_embeddings(\n  (embedding_layer): Embedding(10663, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"trained_model_6 = train_model_with_embeddings(basic_bilstm_with_fasttext_embeddings, 300)","execution_count":74,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning:\n\ntorch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.146280... Val Loss: 0.251066\nEpoch: 1/4... Step: 200... Loss: 0.179410... Val Loss: 0.307596\nEpoch: 1/4... Step: 300... Loss: 0.219573... Val Loss: 0.350433\nEpoch: 1/4... Step: 400... Loss: 0.318640... Val Loss: 0.461883\nEpoch: 1/4... Step: 500... Loss: 1.064413... Val Loss: 0.533259\nEpoch: 1/4... Step: 600... Loss: 0.360991... Val Loss: 0.453801\nEpoch: 1/4... Step: 700... Loss: 3.509548... Val Loss: 0.493007\nEpoch: 1/4... Step: 800... Loss: 0.836383... Val Loss: 0.512989\nEpoch: 1/4... Step: 900... Loss: 0.763261... Val Loss: 0.656232\nEpoch: 1/4... Step: 1000... Loss: 0.151991... Val Loss: 0.492896\nEpoch: 1/4... Step: 1100... Loss: 0.125912... Val Loss: 0.472046\nEpoch: 1/4... Step: 1200... Loss: 1.300927... Val Loss: 0.508908\nEpoch: 1/4... Step: 1300... Loss: 0.298536... Val Loss: 0.456882\nEpoch: 1/4... Step: 1400... Loss: 0.388195... Val Loss: 0.503292\nEpoch: 1/4... Step: 1500... Loss: 0.845753... Val Loss: 0.553771\nEpoch: 1/4... Step: 1600... Loss: 0.416670... Val Loss: 0.553344\nEpoch: 1/4... Step: 1700... Loss: 1.098048... Val Loss: 0.535386\nEpoch: 1/4... Step: 1800... Loss: 0.040188... Val Loss: 0.486282\nEpoch: 1/4... Step: 1900... Loss: 0.013971... Val Loss: 0.435426\nEpoch: 1/4... Step: 2000... Loss: 0.559136... Val Loss: 0.439032\nEpoch: 1/4... Step: 2100... Loss: 0.351766... Val Loss: 0.466028\nEpoch: 1/4... Step: 2200... Loss: 0.355072... Val Loss: 0.455230\nEpoch: 1/4... Step: 2300... Loss: 1.050029... Val Loss: 0.399659\nEpoch: 1/4... Step: 2400... Loss: 0.012547... Val Loss: 0.575226\nEpoch: 2/4... Step: 2500... Loss: 0.948290... Val Loss: 0.295443\nEpoch: 2/4... Step: 2600... Loss: 0.047273... Val Loss: 0.299777\nEpoch: 2/4... Step: 2700... Loss: 1.783020... Val Loss: 0.327851\nEpoch: 2/4... Step: 2800... Loss: 0.371703... Val Loss: 0.272706\nEpoch: 2/4... Step: 2900... Loss: 0.228365... Val Loss: 0.381214\nEpoch: 2/4... Step: 3000... Loss: 0.021874... Val Loss: 0.414881\nEpoch: 2/4... Step: 3100... Loss: 0.688840... Val Loss: 0.422382\nEpoch: 2/4... Step: 3200... Loss: 0.050582... Val Loss: 0.503541\nEpoch: 2/4... Step: 3300... Loss: 0.482682... Val Loss: 0.428413\nEpoch: 2/4... Step: 3400... Loss: 2.312878... Val Loss: 0.601403\nEpoch: 2/4... Step: 3500... Loss: 0.047782... Val Loss: 0.463169\nEpoch: 2/4... Step: 3600... Loss: 2.715049... Val Loss: 0.349054\nEpoch: 2/4... Step: 3700... Loss: 0.326847... Val Loss: 0.367654\nEpoch: 2/4... Step: 3800... Loss: 0.140972... Val Loss: 0.519604\nEpoch: 2/4... Step: 3900... Loss: 0.075213... Val Loss: 0.338364\nEpoch: 2/4... Step: 4000... Loss: 1.504218... Val Loss: 0.160052\nEpoch: 2/4... Step: 4100... Loss: 0.718893... Val Loss: 0.586588\nEpoch: 2/4... Step: 4200... Loss: 0.006584... Val Loss: 0.422804\nEpoch: 2/4... Step: 4300... Loss: 0.123227... Val Loss: 0.049565\nEpoch: 2/4... Step: 4400... Loss: 0.024007... Val Loss: 0.218173\nEpoch: 2/4... Step: 4500... Loss: 0.044144... Val Loss: 0.181108\nEpoch: 2/4... Step: 4600... Loss: 2.352287... Val Loss: 0.133057\nEpoch: 2/4... Step: 4700... Loss: 0.048396... Val Loss: 0.564253\nEpoch: 2/4... Step: 4800... Loss: 0.436709... Val Loss: 0.339245\nEpoch: 2/4... Step: 4900... Loss: 0.270699... Val Loss: 0.134193\nEpoch: 3/4... Step: 5000... Loss: 0.009771... Val Loss: 0.097899\nEpoch: 3/4... Step: 5100... Loss: 0.548578... Val Loss: 0.164871\nEpoch: 3/4... Step: 5200... Loss: 0.034279... Val Loss: 0.041600\nEpoch: 3/4... Step: 5300... Loss: 0.007940... Val Loss: 0.724797\nEpoch: 3/4... Step: 5400... Loss: 0.003040... Val Loss: 0.036357\nEpoch: 3/4... Step: 5500... Loss: 1.022544... Val Loss: 0.046294\nEpoch: 3/4... Step: 5600... Loss: 0.537342... Val Loss: 0.048239\nEpoch: 3/4... Step: 5700... Loss: 0.110944... Val Loss: 0.120487\nEpoch: 3/4... Step: 5800... Loss: 0.098704... Val Loss: 0.060697\nEpoch: 3/4... Step: 5900... Loss: 0.010682... Val Loss: 0.074556\nEpoch: 3/4... Step: 6000... Loss: 2.063203... Val Loss: 0.089372\nEpoch: 3/4... Step: 6100... Loss: 0.127315... Val Loss: 0.398560\nEpoch: 3/4... Step: 6200... Loss: 0.016780... Val Loss: 0.044664\nEpoch: 3/4... Step: 6300... Loss: 0.037898... Val Loss: 0.040350\nEpoch: 3/4... Step: 6400... Loss: 0.296420... Val Loss: 0.052917\nEpoch: 3/4... Step: 6500... Loss: 0.487504... Val Loss: 0.048595\nEpoch: 3/4... Step: 6600... Loss: 0.033546... Val Loss: 0.109527\nEpoch: 3/4... Step: 6700... Loss: 0.159328... Val Loss: 0.128645\nEpoch: 3/4... Step: 6800... Loss: 0.984756... Val Loss: 0.044436\nEpoch: 3/4... Step: 6900... Loss: 0.005145... Val Loss: 0.066482\nEpoch: 3/4... Step: 7000... Loss: 0.033589... Val Loss: 0.068143\nEpoch: 3/4... Step: 7100... Loss: 0.014297... Val Loss: 0.091051\nEpoch: 3/4... Step: 7200... Loss: 0.068291... Val Loss: 0.102874\nEpoch: 3/4... Step: 7300... Loss: 0.023756... Val Loss: 0.055951\nEpoch: 3/4... Step: 7400... Loss: 0.017728... Val Loss: 0.043935\nEpoch: 4/4... Step: 7500... Loss: 0.330997... Val Loss: 0.015738\nEpoch: 4/4... Step: 7600... Loss: 0.002267... Val Loss: 0.016352\nEpoch: 4/4... Step: 7700... Loss: 0.181662... Val Loss: 0.017859\nEpoch: 4/4... Step: 7800... Loss: 0.005589... Val Loss: 0.011329\nEpoch: 4/4... Step: 7900... Loss: 0.020267... Val Loss: 0.043825\nEpoch: 4/4... Step: 8000... Loss: 0.006536... Val Loss: 0.039001\nEpoch: 4/4... Step: 8100... Loss: 0.006116... Val Loss: 0.038112\nEpoch: 4/4... Step: 8200... Loss: 1.183385... Val Loss: 0.123493\nEpoch: 4/4... Step: 8300... Loss: 0.008100... Val Loss: 0.044363\nEpoch: 4/4... Step: 8400... Loss: 0.003029... Val Loss: 0.022282\nEpoch: 4/4... Step: 8500... Loss: 0.597564... Val Loss: 0.035963\nEpoch: 4/4... Step: 8600... Loss: 0.564105... Val Loss: 0.035458\nEpoch: 4/4... Step: 8700... Loss: 0.005877... Val Loss: 0.014288\nEpoch: 4/4... Step: 8800... Loss: 0.010887... Val Loss: 0.009461\nEpoch: 4/4... Step: 8900... Loss: 0.003523... Val Loss: 0.012975\nEpoch: 4/4... Step: 9000... Loss: 0.100848... Val Loss: 0.037811\nEpoch: 4/4... Step: 9100... Loss: 0.157752... Val Loss: 0.035193\nEpoch: 4/4... Step: 9200... Loss: 0.193304... Val Loss: 0.026308\nEpoch: 4/4... Step: 9300... Loss: 0.253092... Val Loss: 0.040598\nEpoch: 4/4... Step: 9400... Loss: 0.017263... Val Loss: 0.028816\nEpoch: 4/4... Step: 9500... Loss: 0.097358... Val Loss: 0.018446\nEpoch: 4/4... Step: 9600... Loss: 0.014125... Val Loss: 0.020020\nEpoch: 4/4... Step: 9700... Loss: 0.077566... Val Loss: 0.024197\nEpoch: 4/4... Step: 9800... Loss: 0.007366... Val Loss: 0.015436\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(trained_model_6, test_loader)\nvalid_acc = predict_model_accuracy(trained_model_6, valid_loader)\n\nprint('Basic bi-lstm model with FastText embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":75,"outputs":[{"output_type":"stream","text":"Test loss: 0.293\naccuracy: 0.883\nTest loss: 0.353\naccuracy: 0.887\nBasic bi-lstm model with FastText embeddings\nTest accuracy :  0.883495145631068\nvalid accuracy :  0.8867313915857605\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#f5ce73;text-align:center\">Bi-LSTM + Crisis embeddings</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_bilstm_with_crisis_embeddings = Basic_BiLSTM_with_embeddings(vocab_size, output_size, 300, hidden_dim, n_layers, torch.Tensor(crisis_embedding_matrix))\nprint(basic_bilstm_with_crisis_embeddings)","execution_count":77,"outputs":[{"output_type":"stream","text":"Basic_BiLSTM_with_embeddings(\n  (embedding_layer): Embedding(10663, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained_model_7 = train_model_with_embeddings(basic_bilstm_with_crisis_embeddings, 300)","execution_count":78,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning:\n\ntorch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.094532... Val Loss: 0.268131\nEpoch: 1/4... Step: 200... Loss: 0.146645... Val Loss: 0.297151\nEpoch: 1/4... Step: 300... Loss: 0.220529... Val Loss: 0.331695\nEpoch: 1/4... Step: 400... Loss: 0.345446... Val Loss: 0.424706\nEpoch: 1/4... Step: 500... Loss: 1.430809... Val Loss: 0.406173\nEpoch: 1/4... Step: 600... Loss: 0.291818... Val Loss: 0.384810\nEpoch: 1/4... Step: 700... Loss: 1.361339... Val Loss: 0.415997\nEpoch: 1/4... Step: 800... Loss: 1.279892... Val Loss: 0.449101\nEpoch: 1/4... Step: 900... Loss: 0.855032... Val Loss: 0.517475\nEpoch: 1/4... Step: 1000... Loss: 0.121199... Val Loss: 0.402679\nEpoch: 1/4... Step: 1100... Loss: 0.135334... Val Loss: 0.399379\nEpoch: 1/4... Step: 1200... Loss: 1.677432... Val Loss: 0.424925\nEpoch: 1/4... Step: 1300... Loss: 0.191367... Val Loss: 0.359589\nEpoch: 1/4... Step: 1400... Loss: 0.352153... Val Loss: 0.438565\nEpoch: 1/4... Step: 1500... Loss: 1.738157... Val Loss: 0.505343\nEpoch: 1/4... Step: 1600... Loss: 0.364972... Val Loss: 0.482528\nEpoch: 1/4... Step: 1700... Loss: 1.053888... Val Loss: 0.505220\nEpoch: 1/4... Step: 1800... Loss: 0.039476... Val Loss: 0.456771\nEpoch: 1/4... Step: 1900... Loss: 0.066432... Val Loss: 0.389939\nEpoch: 1/4... Step: 2000... Loss: 0.396070... Val Loss: 0.439672\nEpoch: 1/4... Step: 2100... Loss: 0.321357... Val Loss: 0.462584\nEpoch: 1/4... Step: 2200... Loss: 0.344998... Val Loss: 0.463444\nEpoch: 1/4... Step: 2300... Loss: 1.377782... Val Loss: 0.456683\nEpoch: 1/4... Step: 2400... Loss: 0.558666... Val Loss: 0.476384\nEpoch: 2/4... Step: 2500... Loss: 1.087093... Val Loss: 0.395012\nEpoch: 2/4... Step: 2600... Loss: 0.287043... Val Loss: 0.459372\nEpoch: 2/4... Step: 2700... Loss: 1.703240... Val Loss: 0.287673\nEpoch: 2/4... Step: 2800... Loss: 0.214717... Val Loss: 0.246655\nEpoch: 2/4... Step: 2900... Loss: 0.274434... Val Loss: 0.297758\nEpoch: 2/4... Step: 3000... Loss: 0.059210... Val Loss: 0.273081\nEpoch: 2/4... Step: 3100... Loss: 0.376053... Val Loss: 0.295140\nEpoch: 2/4... Step: 3200... Loss: 0.247670... Val Loss: 0.483450\nEpoch: 2/4... Step: 3300... Loss: 0.490021... Val Loss: 0.405517\nEpoch: 2/4... Step: 3400... Loss: 0.873065... Val Loss: 0.623674\nEpoch: 2/4... Step: 3500... Loss: 0.264960... Val Loss: 0.415697\nEpoch: 2/4... Step: 3600... Loss: 2.590085... Val Loss: 0.284490\nEpoch: 2/4... Step: 3700... Loss: 0.331155... Val Loss: 0.387051\nEpoch: 2/4... Step: 3800... Loss: 0.635432... Val Loss: 0.679099\nEpoch: 2/4... Step: 3900... Loss: 0.179928... Val Loss: 0.610902\nEpoch: 2/4... Step: 4000... Loss: 1.474950... Val Loss: 0.171688\nEpoch: 2/4... Step: 4100... Loss: 1.236409... Val Loss: 0.524929\nEpoch: 2/4... Step: 4200... Loss: 0.076167... Val Loss: 0.628720\nEpoch: 2/4... Step: 4300... Loss: 0.093364... Val Loss: 0.127856\nEpoch: 2/4... Step: 4400... Loss: 0.146928... Val Loss: 0.262752\nEpoch: 2/4... Step: 4500... Loss: 0.036105... Val Loss: 0.454614\nEpoch: 2/4... Step: 4600... Loss: 1.944751... Val Loss: 0.581971\nEpoch: 2/4... Step: 4700... Loss: 0.679589... Val Loss: 0.759429\nEpoch: 2/4... Step: 4800... Loss: 0.373599... Val Loss: 0.894244\nEpoch: 2/4... Step: 4900... Loss: 0.144749... Val Loss: 0.288528\nEpoch: 3/4... Step: 5000... Loss: 0.036256... Val Loss: 0.772049\nEpoch: 3/4... Step: 5100... Loss: 0.252308... Val Loss: 0.380174\nEpoch: 3/4... Step: 5200... Loss: 0.040283... Val Loss: 0.166436\nEpoch: 3/4... Step: 5300... Loss: 1.193910... Val Loss: 0.674618\nEpoch: 3/4... Step: 5400... Loss: 0.527520... Val Loss: 0.190444\nEpoch: 3/4... Step: 5500... Loss: 0.287495... Val Loss: 0.160550\nEpoch: 3/4... Step: 5600... Loss: 0.902523... Val Loss: 0.236122\nEpoch: 3/4... Step: 5700... Loss: 0.019003... Val Loss: 0.166861\nEpoch: 3/4... Step: 5800... Loss: 0.175060... Val Loss: 0.214177\nEpoch: 3/4... Step: 5900... Loss: 0.030306... Val Loss: 0.134137\nEpoch: 3/4... Step: 6000... Loss: 0.431020... Val Loss: 0.354877\nEpoch: 3/4... Step: 6100... Loss: 1.102748... Val Loss: 0.869851\nEpoch: 3/4... Step: 6200... Loss: 0.011943... Val Loss: 0.333446\nEpoch: 3/4... Step: 6300... Loss: 0.040761... Val Loss: 0.744231\nEpoch: 3/4... Step: 6400... Loss: 0.554614... Val Loss: 0.522839\nEpoch: 3/4... Step: 6500... Loss: 0.511833... Val Loss: 0.088004\nEpoch: 3/4... Step: 6600... Loss: 0.018408... Val Loss: 0.119928\nEpoch: 3/4... Step: 6700... Loss: 0.375489... Val Loss: 0.158281\nEpoch: 3/4... Step: 6800... Loss: 4.365328... Val Loss: 0.140080\nEpoch: 3/4... Step: 6900... Loss: 0.041486... Val Loss: 0.149967\nEpoch: 3/4... Step: 7000... Loss: 0.057478... Val Loss: 0.168249\nEpoch: 3/4... Step: 7100... Loss: 0.025328... Val Loss: 0.354922\nEpoch: 3/4... Step: 7200... Loss: 0.080672... Val Loss: 0.222812\nEpoch: 3/4... Step: 7300... Loss: 0.529029... Val Loss: 0.427952\nEpoch: 3/4... Step: 7400... Loss: 0.079704... Val Loss: 0.294619\nEpoch: 4/4... Step: 7500... Loss: 0.735909... Val Loss: 0.337549\nEpoch: 4/4... Step: 7600... Loss: 0.007749... Val Loss: 0.293139\nEpoch: 4/4... Step: 7700... Loss: 0.458918... Val Loss: 0.118088\nEpoch: 4/4... Step: 7800... Loss: 0.088426... Val Loss: 0.144586\nEpoch: 4/4... Step: 7900... Loss: 0.012727... Val Loss: 0.095283\nEpoch: 4/4... Step: 8000... Loss: 0.022482... Val Loss: 0.063459\nEpoch: 4/4... Step: 8100... Loss: 0.193345... Val Loss: 0.092959\nEpoch: 4/4... Step: 8200... Loss: 1.377006... Val Loss: 0.115175\nEpoch: 4/4... Step: 8300... Loss: 0.008251... Val Loss: 0.132978\nEpoch: 4/4... Step: 8400... Loss: 0.027836... Val Loss: 0.059347\nEpoch: 4/4... Step: 8500... Loss: 0.071123... Val Loss: 0.218172\nEpoch: 4/4... Step: 8600... Loss: 0.894914... Val Loss: 0.472629\nEpoch: 4/4... Step: 8700... Loss: 0.018840... Val Loss: 0.081239\nEpoch: 4/4... Step: 8800... Loss: 0.132082... Val Loss: 0.331641\nEpoch: 4/4... Step: 8900... Loss: 0.010929... Val Loss: 0.231311\nEpoch: 4/4... Step: 9000... Loss: 0.223722... Val Loss: 0.329345\nEpoch: 4/4... Step: 9100... Loss: 0.020252... Val Loss: 0.064256\nEpoch: 4/4... Step: 9200... Loss: 0.362967... Val Loss: 0.046282\nEpoch: 4/4... Step: 9300... Loss: 0.697424... Val Loss: 0.076590\nEpoch: 4/4... Step: 9400... Loss: 0.063695... Val Loss: 0.191172\nEpoch: 4/4... Step: 9500... Loss: 1.733809... Val Loss: 0.078474\nEpoch: 4/4... Step: 9600... Loss: 0.018391... Val Loss: 0.043097\nEpoch: 4/4... Step: 9700... Loss: 0.083094... Val Loss: 0.070195\nEpoch: 4/4... Step: 9800... Loss: 2.214226... Val Loss: 0.212549\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc = predict_model_accuracy(trained_model_7, test_loader)\nvalid_acc = predict_model_accuracy(trained_model_7, valid_loader)\n\nprint('Basic bi-lstm model with Crisis embeddings')\nprint('Test accuracy : ', test_acc)\nprint('valid accuracy : ', valid_acc)","execution_count":79,"outputs":[{"output_type":"stream","text":"Test loss: 0.343\naccuracy: 0.861\nTest loss: 0.365\naccuracy: 0.851\nBasic bi-lstm model with Crisis embeddings\nTest accuracy :  0.86084142394822\nvalid accuracy :  0.8511326860841424\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}