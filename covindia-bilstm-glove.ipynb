{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/twitterdata/finalSentimentdata2.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_df = pd.read_csv('/kaggle/input/twitterdata/finalSentimentdata2.csv')\n\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english')[10:15])\n\ndef punctuation_stopwords_removal(sms):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in sms if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_sms = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_sms\n\nsentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(punctuation_stopwords_removal)\n\nreviews_split = []\nfor i, j in sentiment_df.iterrows():\n    reviews_split.append(j['text'])\n    \nwords = []\nfor review in reviews_split:\n    for word in review:\n        words.append(word)\n        \n        \nfrom collections import Counter\n\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}\n\nencoded_reviews = []\nfor review in reviews_split:\n    encoded_reviews.append([vocab_to_int[word] for word in review])\n    \n    \nlabels_to_int = []\nfor i, j in sentiment_df.iterrows():\n    if j['sentiment']=='joy':\n        labels_to_int.append(1)\n    else:\n        labels_to_int.append(0)\n        \n    \nreviews_len = Counter([len(x) for x in encoded_reviews])\nprint(max(reviews_len))\n\nnon_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\nencoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\nencoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n\ndef pad_features(reviews_int, seq_length):\n    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n    for i, row in enumerate(reviews_int):\n        if len(row)!=0:\n            features[i, -len(row):] = np.array(row)[:seq_length]\n    return features\n\nseq_length = 50\npadded_features= pad_features(encoded_reviews, seq_length)\nprint(padded_features[:2])\n\nsplit_frac = 0.8\nsplit_idx = int(len(padded_features)*split_frac)\n\ntraining_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\ntraining_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# torch.from_numpy creates a tensor data from n-d array\ntrain_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n\nbatch_size = 1\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)\n\ngpu_available = torch.cuda.is_available\n\nif gpu_available:\n    print('Training on GPU')\nelse:\n    print('GPU not available')","execution_count":2,"outputs":[{"output_type":"stream","text":"[\"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n48\n[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0  853  186   20 1079 1457 4429 2201  407 1240 1079\n    15  218  337  167  253  462  337  122  168 4430 4431  140   23  264\n    58  765    3    5  195 1079 2966  274]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0   80 1755 4432 2967\n  4433   86  854 1080 2968 4434 4435    7]]\nTraining on GPU\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass Bidirectional_LSTM_with_Glove_Embeddings(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, embedding_matrix, drop_prob=0.2):\n        super(Bidirectional_LSTM_with_Glove_Embeddings, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding_layer.weight = nn.Parameter(embedding_matrix, requires_grad=False)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, bidirectional=True, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers*2, batch_size, self.hidden_dim).zero())\n        return hidden","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('//kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\n\nnum_words=len(vocab_to_int)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in (vocab_to_int.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1 # either happy or sad\nembedding_dim = 100\nhidden_dim = 256\nn_layers = 2","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1 # either happy or sad\nembedding_dim = 100\nhidden_dim = 256\nn_layers = 2\n\nbilstm_with_glove_embeddings = Bidirectional_LSTM_with_Glove_Embeddings(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, torch.Tensor(embedding_matrix))\nprint(bilstm_with_glove_embeddings)\n\nlr = 0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(bilstm_with_glove_embeddings.parameters(), lr=lr)\n\nepochs = 4\ncount = 0\nprint_every = 100\nclip = 5 \n\nprint(gpu_available)\n\nif gpu_available:\n    bilstm_with_glove_embeddings.cuda()\n\nbilstm_with_glove_embeddings.train()\n\nfor e in range(epochs):\n    # initialize lstm's hidden layer \n    h = bilstm_with_glove_embeddings.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        count += 1\n        if gpu_available:\n            inputs, labels = inputs.cuda(), labels.cuda()\n        h = tuple([each.data for each in h])\n        \n        # training process\n        bilstm_with_glove_embeddings.zero_grad()\n        outputs, h = bilstm_with_glove_embeddings(inputs, h)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm(bilstm_with_glove_embeddings.parameters(), clip)\n        optimizer.step()\n        \n        # print average training losses\n        if count % print_every == 0:\n            val_h = bilstm_with_glove_embeddings.init_hidden(batch_size)\n            val_losses = []\n            bilstm_with_glove_embeddings.eval()\n            for inputs, labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                if gpu_available:\n                    inputs, labels = inputs.cuda(), labels.cuda()\n            outputs, val_h = bilstm_with_glove_embeddings(inputs, val_h)\n            val_loss = criterion(outputs.squeeze(), labels.float())\n            val_losses.append(val_loss.item())\n        \n            bilstm_with_glove_embeddings.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(count),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":8,"outputs":[{"output_type":"stream","text":"Bidirectional_LSTM_with_Glove_Embeddings(\n  (embedding_layer): Embedding(10663, 100)\n  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n<function is_available at 0x7f59a9fdfa70>\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.\n  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n","name":"stderr"},{"output_type":"stream","text":"Epoch: 1/4... Step: 100... Loss: 0.123629... Val Loss: 0.300361\nEpoch: 1/4... Step: 200... Loss: 0.207344... Val Loss: 0.340540\nEpoch: 1/4... Step: 300... Loss: 0.252291... Val Loss: 0.395640\nEpoch: 1/4... Step: 400... Loss: 0.399159... Val Loss: 0.487983\nEpoch: 1/4... Step: 500... Loss: 1.212563... Val Loss: 0.557006\nEpoch: 1/4... Step: 600... Loss: 0.374682... Val Loss: 0.478659\nEpoch: 1/4... Step: 700... Loss: 3.522699... Val Loss: 0.369040\nEpoch: 1/4... Step: 800... Loss: 0.568947... Val Loss: 0.345059\nEpoch: 1/4... Step: 900... Loss: 0.711577... Val Loss: 0.769185\nEpoch: 1/4... Step: 1000... Loss: 0.235918... Val Loss: 0.589461\nEpoch: 1/4... Step: 1100... Loss: 0.028554... Val Loss: 0.597368\nEpoch: 1/4... Step: 1200... Loss: 1.958541... Val Loss: 0.151770\nEpoch: 1/4... Step: 1300... Loss: 0.053413... Val Loss: 0.166294\nEpoch: 1/4... Step: 1400... Loss: 0.221972... Val Loss: 0.228225\nEpoch: 1/4... Step: 1500... Loss: 2.029501... Val Loss: 0.322335\nEpoch: 1/4... Step: 1600... Loss: 0.833623... Val Loss: 0.446113\nEpoch: 1/4... Step: 1700... Loss: 1.256612... Val Loss: 0.228147\nEpoch: 1/4... Step: 1800... Loss: 0.004287... Val Loss: 0.057480\nEpoch: 1/4... Step: 1900... Loss: 0.005057... Val Loss: 0.073585\nEpoch: 1/4... Step: 2000... Loss: 0.177211... Val Loss: 0.048054\nEpoch: 1/4... Step: 2100... Loss: 0.099978... Val Loss: 0.184518\nEpoch: 1/4... Step: 2200... Loss: 0.047632... Val Loss: 0.150664\nEpoch: 1/4... Step: 2300... Loss: 0.621860... Val Loss: 0.106412\nEpoch: 1/4... Step: 2400... Loss: 0.017490... Val Loss: 0.172259\nEpoch: 2/4... Step: 2500... Loss: 0.198853... Val Loss: 0.068098\nEpoch: 2/4... Step: 2600... Loss: 0.019379... Val Loss: 0.061247\nEpoch: 2/4... Step: 2700... Loss: 0.259361... Val Loss: 0.060428\nEpoch: 2/4... Step: 2800... Loss: 0.437439... Val Loss: 0.044880\nEpoch: 2/4... Step: 2900... Loss: 0.024421... Val Loss: 0.076726\nEpoch: 2/4... Step: 3000... Loss: 0.005352... Val Loss: 0.060800\nEpoch: 2/4... Step: 3100... Loss: 0.174079... Val Loss: 0.054831\nEpoch: 2/4... Step: 3200... Loss: 0.014823... Val Loss: 0.050789\nEpoch: 2/4... Step: 3300... Loss: 1.809957... Val Loss: 0.104759\nEpoch: 2/4... Step: 3400... Loss: 2.264310... Val Loss: 0.082187\nEpoch: 2/4... Step: 3500... Loss: 0.196146... Val Loss: 0.082859\nEpoch: 2/4... Step: 3600... Loss: 0.542552... Val Loss: 0.070491\nEpoch: 2/4... Step: 3700... Loss: 0.017058... Val Loss: 0.045573\nEpoch: 2/4... Step: 3800... Loss: 0.024822... Val Loss: 0.213946\nEpoch: 2/4... Step: 3900... Loss: 0.040016... Val Loss: 0.062835\nEpoch: 2/4... Step: 4000... Loss: 1.964250... Val Loss: 0.041014\nEpoch: 2/4... Step: 4100... Loss: 3.481337... Val Loss: 0.073283\nEpoch: 2/4... Step: 4200... Loss: 0.019432... Val Loss: 0.153413\nEpoch: 2/4... Step: 4300... Loss: 0.017024... Val Loss: 0.018708\nEpoch: 2/4... Step: 4400... Loss: 0.013124... Val Loss: 0.045826\nEpoch: 2/4... Step: 4500... Loss: 0.011551... Val Loss: 0.032692\nEpoch: 2/4... Step: 4600... Loss: 0.108750... Val Loss: 0.044426\nEpoch: 2/4... Step: 4700... Loss: 0.006568... Val Loss: 0.047979\nEpoch: 2/4... Step: 4800... Loss: 0.233805... Val Loss: 0.048306\nEpoch: 2/4... Step: 4900... Loss: 0.017819... Val Loss: 0.037258\nEpoch: 3/4... Step: 5000... Loss: 0.031945... Val Loss: 0.039487\nEpoch: 3/4... Step: 5100... Loss: 0.026564... Val Loss: 0.036311\nEpoch: 3/4... Step: 5200... Loss: 0.021025... Val Loss: 0.047690\nEpoch: 3/4... Step: 5300... Loss: 0.021248... Val Loss: 0.030241\nEpoch: 3/4... Step: 5400... Loss: 0.001262... Val Loss: 0.013405\nEpoch: 3/4... Step: 5500... Loss: 1.297627... Val Loss: 0.036353\nEpoch: 3/4... Step: 5600... Loss: 0.155948... Val Loss: 0.025922\nEpoch: 3/4... Step: 5700... Loss: 0.027173... Val Loss: 0.149782\nEpoch: 3/4... Step: 5800... Loss: 0.018756... Val Loss: 0.072336\nEpoch: 3/4... Step: 5900... Loss: 0.006817... Val Loss: 0.045554\nEpoch: 3/4... Step: 6000... Loss: 0.128390... Val Loss: 0.039447\nEpoch: 3/4... Step: 6100... Loss: 0.039549... Val Loss: 0.111334\nEpoch: 3/4... Step: 6200... Loss: 0.019149... Val Loss: 0.034355\nEpoch: 3/4... Step: 6300... Loss: 0.035732... Val Loss: 0.192976\nEpoch: 3/4... Step: 6400... Loss: 0.367462... Val Loss: 0.160139\nEpoch: 3/4... Step: 6500... Loss: 0.270631... Val Loss: 0.041301\nEpoch: 3/4... Step: 6600... Loss: 0.018375... Val Loss: 0.048034\nEpoch: 3/4... Step: 6700... Loss: 0.034025... Val Loss: 0.110703\nEpoch: 3/4... Step: 6800... Loss: 1.025153... Val Loss: 0.025565\nEpoch: 3/4... Step: 6900... Loss: 0.005444... Val Loss: 0.018182\nEpoch: 3/4... Step: 7000... Loss: 2.784145... Val Loss: 0.044628\nEpoch: 3/4... Step: 7100... Loss: 0.005768... Val Loss: 0.020945\nEpoch: 3/4... Step: 7200... Loss: 0.397092... Val Loss: 0.039806\nEpoch: 3/4... Step: 7300... Loss: 0.114126... Val Loss: 0.025128\nEpoch: 3/4... Step: 7400... Loss: 0.291125... Val Loss: 0.075799\nEpoch: 4/4... Step: 7500... Loss: 0.146271... Val Loss: 0.012526\nEpoch: 4/4... Step: 7600... Loss: 0.004316... Val Loss: 0.014440\nEpoch: 4/4... Step: 7700... Loss: 0.085818... Val Loss: 0.028266\nEpoch: 4/4... Step: 7800... Loss: 0.003444... Val Loss: 0.010523\nEpoch: 4/4... Step: 7900... Loss: 0.047283... Val Loss: 0.046335\nEpoch: 4/4... Step: 8000... Loss: 0.007621... Val Loss: 0.059546\nEpoch: 4/4... Step: 8100... Loss: 0.002855... Val Loss: 0.048871\nEpoch: 4/4... Step: 8200... Loss: 0.052353... Val Loss: 0.050223\nEpoch: 4/4... Step: 8300... Loss: 0.005436... Val Loss: 0.027480\nEpoch: 4/4... Step: 8400... Loss: 0.004789... Val Loss: 0.018191\nEpoch: 4/4... Step: 8500... Loss: 0.017970... Val Loss: 0.029511\nEpoch: 4/4... Step: 8600... Loss: 0.553168... Val Loss: 0.028061\nEpoch: 4/4... Step: 8700... Loss: 0.011192... Val Loss: 0.034072\nEpoch: 4/4... Step: 8800... Loss: 0.469736... Val Loss: 0.012486\nEpoch: 4/4... Step: 8900... Loss: 0.006195... Val Loss: 0.023684\nEpoch: 4/4... Step: 9000... Loss: 0.041839... Val Loss: 0.086115\nEpoch: 4/4... Step: 9100... Loss: 0.055594... Val Loss: 0.084633\nEpoch: 4/4... Step: 9200... Loss: 0.044621... Val Loss: 0.050307\nEpoch: 4/4... Step: 9300... Loss: 0.008236... Val Loss: 0.019106\nEpoch: 4/4... Step: 9400... Loss: 0.032601... Val Loss: 0.020945\nEpoch: 4/4... Step: 9500... Loss: 0.008206... Val Loss: 0.019693\nEpoch: 4/4... Step: 9600... Loss: 0.001391... Val Loss: 0.019005\nEpoch: 4/4... Step: 9700... Loss: 0.099661... Val Loss: 0.027393\nEpoch: 4/4... Step: 9800... Loss: 0.003417... Val Loss: 0.007950\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_losses = []\nnum_correct = 0\n\nh = bilstm_with_glove_embeddings.init_hidden(batch_size)\nbilstm_with_glove_embeddings.eval()\n\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    if gpu_available:\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    outputs, h = bilstm_with_glove_embeddings(inputs, h)\n    test_loss = criterion(outputs.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(outputs.squeeze())\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n# printing average statistics\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n    \n# accuracy over all test data\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","execution_count":9,"outputs":[{"output_type":"stream","text":"Test loss: 0.275\nTest accuracy: 0.896\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}